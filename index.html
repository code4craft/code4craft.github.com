
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>代码人生</title>
  <meta name="author" content="黄亿华">

  
  <meta name="description" content="webmagic的由来 一般来说，一个爬虫包括几个部分： 页面下载 页面下载是一个爬虫的基础。下载页面之后才能进行其他后续操作。
链接提取 一般爬虫都会有一些初始的种子URL，但是这些URL对于爬虫是远远不够的。爬虫在爬页面的时候，需要不断发现新的链接。
URL管理 最基础的URL管理， &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://code4craft.github.com/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/ender.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="代码人生" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  

</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">代码人生</a></h1>
  
    <h2>A blog for coder lost in career.</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:code4craft.github.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/07/18/webmagicde-she-ji-ji-zhi-ji-yuan-li/">Webmagic的设计机制及原理</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-07-18T14:52:00+08:00" pubdate data-updated="true">Jul 18<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p><img src="http://code4craft.github.io/images/posts/spider.jpeg" alt="image" /></p>

<h1>webmagic的由来</h1>

<p>一般来说，一个爬虫包括几个部分：</p>

<ul>
<li><p>页面下载</p>

<p>页面下载是一个爬虫的基础。下载页面之后才能进行其他后续操作。</p></li>
<li><p>链接提取</p>

<p>一般爬虫都会有一些初始的种子URL，但是这些URL对于爬虫是远远不够的。爬虫在爬页面的时候，需要不断发现新的链接。</p></li>
<li><p>URL管理</p>

<p>最基础的URL管理，就是对已经爬过的URL和没有爬的URL做区分，防止重复爬取。</p></li>
<li><p>内容分析和持久化</p>

<p>一般来说，我们最终需要的都不是原始的HTML页面。我们需要对爬到的页面进行分析，转化成结构化的数据，并存储下来。</p></li>
</ul>


<p>不同的爬虫，对这几部分的要求是不一样的。</p>

<p>对于通用型的爬虫，例如搜索引擎蜘蛛，需要指对互联网大部分网页无差别进行抓取。这时候难点就在于页面下载和链接管理上&mdash;如果要高效的抓取更多页面，就必须进行更快的下载；同时随着链接数量的增多，需要考虑如果对大规模的链接进行去重和调度，就成了一个很大的问题。一般这些问题都会在大公司有专门的团队去解决，比如这里有一篇来自淘宝的<a href="http://www.searchtb.com/2011/07/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BA%E5%AE%9E%E6%97%B6%E6%8A%93%E5%8F%96%E9%9B%86%E7%BE%A4.html?spm=0.0.0.0.hHzGxv">快速构建实时抓取集群</a>。</p>

<p>而垂直类型的爬虫要解决的问题则不一样，比如想要爬取一些网站的新闻、博客信息，一般抓取数量要求不是很大，难点则在于如何高效的定制一个爬虫，可以精确的抽取出网页的内容，并保存成结构化的数据。这方面需求很多，webmagic就是为了解决这个目的而开发的。</p>

<p>使用Java语言开发爬虫是比较复杂的。虽然Java有很强大的页面下载、HTML分析工具，但是每个都有不小的学习成本，而且这些工具本身都不是专门为爬虫而生，使用起来也没有那么顺手。我曾经有一年的时间都在开发爬虫，重复的开发让人头痛。Java还有一个比较成熟的框架crawler4j，但是它是为通用爬虫而设计的，扩展性差一些，满足不了我的业务需要。我也有过自己开发框架的念头，但是终归觉得抽象的不是很好。直到发现python的爬虫框架scrapy，它将爬虫的生命周期拆分的非常清晰，我参照它进行了模块划分，并用Java的方式去实现了它，于是就有了webmagic。代码已经托管到github，地址是<a href="https://github.com/code4craft/webmagic">https://github.com/code4craft/webmagic</a>。webmagic的目标是，成为一个可以非常方便的进行扩展和二次开发的爬虫框架。</p>

<hr />

<h1>webmagic的模块划分</h1>

<p><img src="http://code4craft.github.io/images/posts/webmagic.png" alt="image" /></p>

<h2>Spider类-核心调度</h2>

<p>Spider的核心处理流程非常简单：</p>

<pre><code>private void processRequest(Request request) {
    Page page = downloader.download(request, this);
    if (page == null) {
        sleep(site.getSleepTime());
        return;
    }
    pageProcessor.process(page);
    addRequest(page);
    for (Pipeline pipeline : pipelines) {
        pipeline.process(page, this);
    }
    sleep(site.getSleepTime());
}
</code></pre>

<h2>Downloader-页面下载</h2>

<p>页面下载是一切爬虫的开始。这方面，<strong>HttpClient</strong>(4.0后整合到HttpCompenent项目中)是不二之选。它支持自定义HTTP头(对于爬虫比较有用的就是User-agent、cookie什么的)、自动redirect、连接复用、设置代理等诸多强大的功能。</p>

<p>webmagic使用了HttpClient 4.0，并封装到了<strong>HttpClientDownloader</strong>。这部分参考了另一个Java爬虫<a href="https://gitcafe.com/laiweiwei/Spiderman"><strong>SpiderMan</strong></a>的部分代码。<strong>SpiderMan</strong>是一个全栈式的Java爬虫，它的设计思想跟webmagic稍有不同，它希望将Java语言的实现隔离，仅仅让用户通过配置就完成一个垂直爬虫，而webmagic则是为Java开发者可以方便的进行开发而设计的。</p>

<p>webmagic还有一个好玩的功能<strong>FileDownloader</strong>。如果你不知道怎么分析页面，只是想先把它下载下来，那么可以使用FilePipeline将页面全文保存，下次运行的时候，使用FileDownloader，则可以直接从本地文件载入页面，并进行处理。对于其他模块，这跟实时下载并处理没有什么不同。</p>

<h2>PageProcessor-页面分析及链接抽取</h2>

<p>这里说的页面分析主要指HTML页面的分析。页面分析可以说是垂直爬虫最复杂的一部分，webmagic的大部分工作都围绕它展开。Java世界主要有几款比较方便的分析工具：</p>

<p><strong>Jsoup</strong></p>

<p><strong>SaxParser for html</strong></p>

<p><strong>HtmlParser</strong></p>

<p><strong>HtmlCleaner</strong></p>

<p>Selector是webmagic的模块中我最得意的部分，这里整合了xpath和正则表达式，并可以进行链式的抽取，很容易就实现强大的功能。例如，抽取一个页面某个区域的所有包含&#8221;blog&#8221;的链接，我可以这样写：</p>

<pre><code>page.getHtml().xpath("//div[@class='title']").links().regex(".*blog.*").toStrings();
</code></pre>

<p>这里用到了两个文本处理的DSL(Domain-specific language，领域特定语言)，一个是正则表达式，一个是xpath。</p>

<h2>Scheduler-URL管理</h2>

<p>URL管理的问题可大可小。对于小规模的抓取，URL管理是很简单的。我们只需要将待抓取URL和未抓取URL分开保存，并进行去重即可。使用JDK内置的集合类型Set、List或者Queue都可以满足需要。如果我们要进行多线程抓取，则可以选择线程安全的容器，例如LinkedBlockingQueue以及ConcurrentHashMap。</p>

<p>因为小规模的URL管理非常简单，很多框架都并不将其抽象为一个模块，而是直接融入到代码中。但是实际上，抽象出Scheduler模块，会使得框架的解耦程序上升一个档次，这也是我从scrapy中学到的。</p>

<p>在webmagic的设计中，除了Scheduler模块，其他的处理-从下载、解析到持久化，每个页面都是互相独立的。排除去重的因素，URL管理天生就是一个队列，我们可以很方便的用分布式的队列工具去扩展它，也可以基于mysql、redis或者mongodb这样的存储工具来构造一个队列，这样构建一个分布式的爬虫就轻而易举了。</p>

<p>webmagic目前有两个Scheduler的实现，<strong>QueueScheduler</strong>是一个简单的内存队列，速度较快，并且是线程安全的，<strong>FileCacheQueueScheduler</strong>则是一个文件队列，它可以在较长的下载任务，中途停止后，下次执行仍然从中止的URL开始继续爬取。</p>

<h2>Pipeline-离线处理和持久化</h2>

<p>Pipeline其实也是比较有争议的一部分。大家都知道持久化的重要性，但是很多框架都选择直接在页面抽取的时候将持久化一起完成，例如crawer4j。但是Pipeline真正的好处是，将页面的在线分析和离线处理拆分开来，可以在一些线程里进行下载，另一些线程里进行处理和持久化。</p>

<p>你可以扩展Pipeline来实现抽取结果的持久化，将其保存到你想要保存的地方-本地文件、数据库、mongodb等等。Pipeline的处理目前还是在线的，但是修改为离线的也并不困难。</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/07/07/monkeysocks-arch/">Monkeysocks架构规划</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-07-07T21:35:00+08:00" pubdate data-updated="true">Jul 7<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><blockquote><p>monkeysocks的目标是为开发以及测试提供一个稳定的环境。它使用socks代理，将录制网络流量并本地保存，并在测试时将其重放。</p></blockquote>

<h2>jsocks的改造</h2>

<p>首先对公司一个项目进行了代理，测试结果：从开始启动到完成，只有4.7M的网络流量，本地空间开销不是问题。</p>

<p>今天把jsocks修改了下，将build工具换成了maven，并独立成了项目<a href="https://github.com/code4craft/monkeysocks/jsocks">https://github.com/code4craft/jsocks</a>。后来算是把record和replay功能做完了，开始研究各种协议replay的可能性。</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/07/07/monkeysocks-arch/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/07/06/monkeysocks/">Monkeysocks开发日志</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-07-06T16:09:00+08:00" pubdate data-updated="true">Jul 6<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><blockquote><p>monkeysocks的目标是为开发以及测试提供一个稳定的环境。</p></blockquote>

<h4>2013-7-5 动机</h4>

<p>前几天听说公司的测试团队在鼓捣数据固化的东西，说白了就是在测试启动时构建一个临时性的数据库，操作完之后再销毁，这样的好处是不造成测试副作用，同时屏蔽环境的差异。</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/07/06/monkeysocks/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/07/03/show-in-github/">玩转github之&#8211;神啊满足我的虚荣心吧</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-07-03T07:47:00+08:00" pubdate data-updated="true">Jul 3<span>rd</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h3>github版简历</h3>

<p><a href="http://resume.github.io/">http://resume.github.io/</a>上有这个东东，但是样式太难看了。看到一个挺不错的模板<a href="https://github.com/hit9/GhResume">https://github.com/hit9/GhResume</a>，就给用上了。我的简历：
<a href="http://code4craft.github.io/GhResume/">http://code4craft.github.io/GhResume/</a></p>

<h3>关注star</h3>

<p>最近做的项目在github每天会有几个star，出于虚荣心嘛，经常忍不住就会去看看，有人star了没？有人fork了没？</p>

<p>每天看太麻烦了，干脆做成一个chrome插件，带桌面通知，有新star提醒，岂不开心？</p>

<p>于是有了<a href="https://github.com/code4craft/exciting"><strong>exciting</strong></a>！ 哥也会做chrome插件了！液！</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/06/28/todolist/">一个shell下的todolist</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-06-28T10:17:00+08:00" pubdate data-updated="true">Jun 28<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>使用和文件保存都挺简单，试试看吧！</p>

<p><a href="http://todotxt.com/">http://todotxt.com/</a>貌似还能同步到手机端？</p>

<p>下载之后，</p>

<pre><code>ln -s xxx/todo.sh /usr/local/bin/todo
</code></pre>

<p>貌似这样会把东西存到/usr/local/bin/todo.txt里？不管了！</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/06/27/yong-pythonxie-liao-ge-xiao-gong-ju/">写了个快捷保存文本的shell工具</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-06-27T22:14:00+08:00" pubdate data-updated="true">Jun 27<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>作为一个Java程序员，对脚本语言程序员自己随时写些小工具，已经羡慕很久了，于是最终开始了脚本语言的学习。</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/06/27/yong-pythonxie-liao-ge-xiao-gong-ju/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/06/23/%5B%3F%5D-ci-jvmdiao-you-de-jing-guo/">BlackHole开发日记-一次压力测试及JVM调优的经过</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-06-23T06:44:00+08:00" pubdate data-updated="true">Jun 23<span>rd</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>BlackHole开发很久了，目前稳定性、性能都还可以了，但是作为一个Java程序，内存开销一直是硬伤，动不动100M内存下去了，对于单机用户实在是不太友好。</p>

<p>怎么办？优化先从分析开始！</p>

<h3>获取内存信息</h3>

<p>获取内存信息一般使用jmap。</p>

<pre><code>jmap -histo pid
</code></pre>

<p>这种方式获取到得比较简略，我们可以先把内存dump下来，再进行离线分析。jhat是一个离线内存分析工具，会开启一个web服务以供展示。</p>

<pre><code>jmap -dump:file=dumpfile pid
jhat -J-Xmx512m dumpfile
</code></pre>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/06/23/%5B%3F%5D-ci-jvmdiao-you-de-jing-guo/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/06/22/huan-dian-nao-liao/">换电脑了</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-06-22T08:57:00+08:00" pubdate data-updated="true">Jun 22<span>nd</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>公司政策，每个人给6500软妹币报销，可以买电脑。因为刚好WWDC开完，老AIR降价，本着好用和省钱的标准，就买了老AIR一台，7388包发票。</p>

<p>换到AIR最不习惯的是屏幕，PRO的屏幕是1280*800，AIR的屏幕是1440x900，所以字总是特小，原来12pt的字体非要14pt才能显示出来，MAC还没法设置默认字体，只能装了个Tinker Tool。设置之后，各种丑和切边…没办法，办公用，眼睛不累是原则！</p>

<p>换电脑之后，得把老的东西弄过来。这次RVM倒是一次成功了，换了gem的source为淘宝的，更新还挺快的。octpress又重新装了一次，好像ruby的包好多都是安装到一个目录下可用的？最后只能把原来的octpress连同博客一起拷贝过来了。</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/06/10/shi-yong-webmagiczhua-qu-ye-mian-bing-bao-cun-wei-wordpresswen-jian/">使用webmagic抓取页面并保存为wordpress文件</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-06-10T18:05:00+08:00" pubdate data-updated="true">Jun 10<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>之前做过一年的爬虫，当年功力不够，写的代码都是一点一点往上加。后来看了下据说是最优秀的爬虫<a href="http://www.oschina.net/p/scrapy"><code>scrapy</code></a>的结构，山寨了一个Java版的爬虫框架。</p>

<p>这个框架也分为Spider、Schedular、Downloader、Pipeline几个模块。此外有一个Selector，整合了常用的抽取技术(正则、xpath)，支持链式调用以及单复数切换，因为受够了各种抽取的正则，在抽取上多下了一点功夫。</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/06/10/shi-yong-webmagiczhua-qu-ye-mian-bing-bao-cun-wei-wordpresswen-jian/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/06/07/2013nian-ban-nian-zong-jie-ji-ji-hua/">2013年半年总结及计划</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-06-07T07:08:00+08:00" pubdate data-updated="true">Jun 7<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>2013年前半年搞Scrum，工作忙了不少，但是却是瞎忙的多。</p>

<p>持续维护<code>BlackHole</code>半年时间，github上有了 <strong>24</strong> 个star，半年来最大的成果。</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/06/07/2013nian-ban-nian-zong-jie-ji-ji-hua/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/blog/page/2/">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
  </div>
</div>
<aside class="sidebar">
  
    
<section>
  <h1>新浪微博</h1>
  <ul id="weibo">
    <li>
      <iframe 
        width="100%" 
        height="550" 
        class="share_self" 
        frameborder="0" 
        scrolling="no" 
        src="http://widget.weibo.com/weiboshow/index.php?width=0&height=550&ptype=1&speed=0&skin=&isTitle=0&noborder=1&isWeibo=1&isFans=&uid=1487828712&verifier=a3843d95">
      </iframe>
    </li>
  </ul>
</section>

<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2013/07/18/webmagicde-she-ji-ji-zhi-ji-yuan-li/">webmagic的设计机制及原理</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/07/07/monkeysocks-arch/">monkeysocks架构规划</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/07/06/monkeysocks/">monkeysocks开发日志</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/07/03/show-in-github/">玩转github之&#8211;神啊满足我的虚荣心吧</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/06/28/todolist/">一个shell下的todolist</a>
      </li>
    
  </ul>
</section>






  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2013 - 黄亿华 -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
