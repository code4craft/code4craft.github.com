
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>webmagic的设计机制及原理 - 代码人生</title>
  <meta name="author" content="黄亿华">

  
  <meta name="description" content="webmagic的由来 一般来说，一个爬虫包括几个部分： 页面下载 页面下载是一个爬虫的基础。下载页面之后才能进行其他后续操作。
链接提取 一般爬虫都会有一些初始的种子URL，但是这些URL对于爬虫是远远不够的。爬虫在爬页面的时候，需要不断发现新的链接。
URL管理 最基础的URL管理， &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://code4craft.github.com/blog/2013/07/18/webmagicde-she-ji-ji-zhi-ji-yuan-li/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/ender.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="代码人生" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  

</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">代码人生</a></h1>
  
    <h2>A blog for coder lost in career.</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:code4craft.github.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">Webmagic的设计机制及原理</h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-07-18T14:52:00+08:00" pubdate data-updated="true">Jul 18<span>th</span>, 2013</time>
        
      </p>
    
  </header>


<div class="entry-content"><p><img src="http://code4craft.github.io/images/posts/spider.jpeg" alt="image" /></p>

<h1>webmagic的由来</h1>

<p>一般来说，一个爬虫包括几个部分：</p>

<ul>
<li><p>页面下载</p>

<p>页面下载是一个爬虫的基础。下载页面之后才能进行其他后续操作。</p></li>
<li><p>链接提取</p>

<p>一般爬虫都会有一些初始的种子URL，但是这些URL对于爬虫是远远不够的。爬虫在爬页面的时候，需要不断发现新的链接。</p></li>
<li><p>URL管理</p>

<p>最基础的URL管理，就是对已经爬过的URL和没有爬的URL做区分，防止重复爬取。</p></li>
<li><p>内容分析和持久化</p>

<p>一般来说，我们最终需要的都不是原始的HTML页面。我们需要对爬到的页面进行分析，转化成结构化的数据，并存储下来。</p></li>
</ul>


<p>不同的爬虫，对这几部分的要求是不一样的。</p>

<p>对于通用型的爬虫，例如搜索引擎蜘蛛，需要指对互联网大部分网页无差别进行抓取。这时候难点就在于页面下载和链接管理上&mdash;如果要高效的抓取更多页面，就必须进行更快的下载；同时随着链接数量的增多，需要考虑如果对大规模的链接进行去重和调度，就成了一个很大的问题。一般这些问题都会在大公司有专门的团队去解决，比如这里有一篇来自淘宝的<a href="http://www.searchtb.com/2011/07/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BA%E5%AE%9E%E6%97%B6%E6%8A%93%E5%8F%96%E9%9B%86%E7%BE%A4.html?spm=0.0.0.0.hHzGxv">快速构建实时抓取集群</a>。</p>

<p>而垂直类型的爬虫要解决的问题则不一样，比如想要爬取一些网站的新闻、博客信息，一般抓取数量要求不是很大，难点则在于如何高效的定制一个爬虫，可以精确的抽取出网页的内容，并保存成结构化的数据。这方面需求很多，webmagic就是为了解决这个目的而开发的。</p>

<p>使用Java语言开发爬虫是比较复杂的。虽然Java有很强大的页面下载、HTML分析工具，但是每个都有不小的学习成本，而且这些工具本身都不是专门为爬虫而生，使用起来也没有那么顺手。我曾经有一年的时间都在开发爬虫，重复的开发让人头痛。Java还有一个比较成熟的框架crawler4j，但是它是为通用爬虫而设计的，扩展性差一些，满足不了我的业务需要。我也有过自己开发框架的念头，但是终归觉得抽象的不是很好。直到发现python的爬虫框架scrapy，它将爬虫的生命周期拆分的非常清晰，我参照它进行了模块划分，并用Java的方式去实现了它，于是就有了webmagic。代码已经托管到github，地址是<a href="https://github.com/code4craft/webmagic">https://github.com/code4craft/webmagic</a>。webmagic的目标是，成为一个可以非常方便的进行扩展和二次开发的爬虫框架。</p>

<hr />

<h1>webmagic的模块划分</h1>

<p><img src="http://code4craft.github.io/images/posts/webmagic.png" alt="image" /></p>

<h2>Spider类-核心调度</h2>

<p>Spider的核心处理流程非常简单：</p>

<pre><code>private void processRequest(Request request) {
    Page page = downloader.download(request, this);
    if (page == null) {
        sleep(site.getSleepTime());
        return;
    }
    pageProcessor.process(page);
    addRequest(page);
    for (Pipeline pipeline : pipelines) {
        pipeline.process(page, this);
    }
    sleep(site.getSleepTime());
}
</code></pre>

<h2>Downloader-页面下载</h2>

<p>页面下载是一切爬虫的开始。这方面，<strong>HttpClient</strong>(4.0后整合到HttpCompenent项目中)是不二之选。它支持自定义HTTP头(对于爬虫比较有用的就是User-agent、cookie什么的)、自动redirect、连接复用、设置代理等诸多强大的功能。</p>

<p>webmagic使用了HttpClient 4.0，并封装到了<strong>HttpClientDownloader</strong>。这部分参考了另一个Java爬虫<a href="https://gitcafe.com/laiweiwei/Spiderman"><strong>SpiderMan</strong></a>的部分代码。<strong>SpiderMan</strong>是一个全栈式的Java爬虫，它的设计思想跟webmagic稍有不同，它希望将Java语言的实现隔离，仅仅让用户通过配置就完成一个垂直爬虫，而webmagic则是为Java开发者可以方便的进行开发而设计的。</p>

<p>webmagic还有一个好玩的功能<strong>FileDownloader</strong>。如果你不知道怎么分析页面，只是想先把它下载下来，那么可以使用FilePipeline将页面全文保存，下次运行的时候，使用FileDownloader，则可以直接从本地文件载入页面，并进行处理。对于其他模块，这跟实时下载并处理没有什么不同。</p>

<h2>PageProcessor-页面分析及链接抽取</h2>

<p>这里说的页面分析主要指HTML页面的分析。页面分析可以说是垂直爬虫最复杂的一部分，webmagic的大部分工作都围绕它展开。Java世界主要有几款比较方便的分析工具：</p>

<p><strong>Jsoup</strong></p>

<p><strong>SaxParser for html</strong></p>

<p><strong>HtmlParser</strong></p>

<p><strong>HtmlCleaner</strong></p>

<p>Selector是webmagic的模块中我最得意的部分，这里整合了xpath和正则表达式，并可以进行链式的抽取，很容易就实现强大的功能。例如，抽取一个页面某个区域的所有包含&#8221;blog&#8221;的链接，我可以这样写：</p>

<pre><code>page.getHtml().xpath("//div[@class='title']").links().regex(".*blog.*").toStrings();
</code></pre>

<p>这里用到了两个文本处理的DSL(Domain-specific language，领域特定语言)，一个是正则表达式，一个是xpath。</p>

<h2>Scheduler-URL管理</h2>

<p>URL管理的问题可大可小。对于小规模的抓取，URL管理是很简单的。我们只需要将待抓取URL和未抓取URL分开保存，并进行去重即可。使用JDK内置的集合类型Set、List或者Queue都可以满足需要。如果我们要进行多线程抓取，则可以选择线程安全的容器，例如LinkedBlockingQueue以及ConcurrentHashMap。</p>

<p>因为小规模的URL管理非常简单，很多框架都并不将其抽象为一个模块，而是直接融入到代码中。但是实际上，抽象出Scheduler模块，会使得框架的解耦程序上升一个档次，这也是我从scrapy中学到的。</p>

<p>在webmagic的设计中，除了Scheduler模块，其他的处理-从下载、解析到持久化，每个页面都是互相独立的。排除去重的因素，URL管理天生就是一个队列，我们可以很方便的用分布式的队列工具去扩展它，也可以基于mysql、redis或者mongodb这样的存储工具来构造一个队列，这样构建一个分布式的爬虫就轻而易举了。</p>

<p>webmagic目前有两个Scheduler的实现，<strong>QueueScheduler</strong>是一个简单的内存队列，速度较快，并且是线程安全的，<strong>FileCacheQueueScheduler</strong>则是一个文件队列，它可以在较长的下载任务，中途停止后，下次执行仍然从中止的URL开始继续爬取。</p>

<h2>Pipeline-离线处理和持久化</h2>

<p>Pipeline其实也是比较有争议的一部分。大家都知道持久化的重要性，但是很多框架都选择直接在页面抽取的时候将持久化一起完成，例如crawer4j。但是Pipeline真正的好处是，将页面的在线分析和离线处理拆分开来，可以在一些线程里进行下载，另一些线程里进行处理和持久化。</p>

<p>你可以扩展Pipeline来实现抽取结果的持久化，将其保存到你想要保存的地方-本地文件、数据库、mongodb等等。Pipeline的处理目前还是在线的，但是修改为离线的也并不困难。</p>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">黄亿华</span></span>

      








  


<time datetime="2013-07-18T14:52:00+08:00" pubdate data-updated="true">Jul 18<span>th</span>, 2013</time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/pa-chong/'>爬虫</a>
  
</span>


    </p>
    
      <div class="sharing">
  
  <a href="http://twitter.com/share" class="twitter-share-button" data-url="http://code4craft.github.com/blog/2013/07/18/webmagicde-she-ji-ji-zhi-ji-yuan-li/" data-via="" data-counturl="http://code4craft.github.com/blog/2013/07/18/webmagicde-she-ji-ji-zhi-ji-yuan-li/" >Tweet</a>
  
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2013/07/07/monkeysocks-arch/" title="Previous Post: monkeysocks架构规划">&laquo; monkeysocks架构规划</a>
      
      
    </p>
  </footer>
</article>

<section imagesd="comment">
<h1>Add a comment</h1>
<!-- Duoshuo Comment BEGIN -->
	<div class="ds-thread"></div>
<script type="text/javascript">
var duoshuoQuery = {short_name:"code4craft"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = 'http://static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		|| document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>
<!-- Duoshuo Comment END -->

</section>

</div>

<aside class="sidebar">
  
    
<section>
  <h1>新浪微博</h1>
  <ul id="weibo">
    <li>
      <iframe 
        width="100%" 
        height="550" 
        class="share_self" 
        frameborder="0" 
        scrolling="no" 
        src="http://widget.weibo.com/weiboshow/index.php?width=0&height=550&ptype=1&speed=0&skin=&isTitle=0&noborder=1&isWeibo=1&isFans=&uid=1487828712&verifier=a3843d95">
      </iframe>
    </li>
  </ul>
</section>

<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2013/07/18/webmagicde-she-ji-ji-zhi-ji-yuan-li/">webmagic的设计机制及原理</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/07/07/monkeysocks-arch/">monkeysocks架构规划</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/07/06/monkeysocks/">monkeysocks开发日志</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/07/03/show-in-github/">玩转github之--神啊满足我的虚荣心吧</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/06/28/todolist/">一个shell下的todolist</a>
      </li>
    
  </ul>
</section>






  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2013 - 黄亿华 -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
