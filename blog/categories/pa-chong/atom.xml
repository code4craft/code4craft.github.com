<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 爬虫 | 代码人生]]></title>
  <link href="http://code4craft.github.com/blog/categories/pa-chong/atom.xml" rel="self"/>
  <link href="http://code4craft.github.com/"/>
  <updated>2013-07-20T07:33:28+08:00</updated>
  <id>http://code4craft.github.com/</id>
  <author>
    <name><![CDATA[黄亿华]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[webmagic的设计机制及原理-如何开发一个Java爬虫]]></title>
    <link href="http://code4craft.github.com/blog/2013/07/18/webmagicde-she-ji-ji-zhi-ji-yuan-li/"/>
    <updated>2013-07-18T14:52:00+08:00</updated>
    <id>http://code4craft.github.com/blog/2013/07/18/webmagicde-she-ji-ji-zhi-ji-yuan-li</id>
    <content type="html"><![CDATA[<p><img src="http://code4craft.github.io/images/posts/spider.jpeg" alt="image" /></p>

<h2>webmagic的目标</h2>

<p>一般来说，一个爬虫包括几个部分：</p>

<ul>
<li><p>页面下载</p>

<p>页面下载是一个爬虫的基础。下载页面之后才能进行其他后续操作。</p></li>
<li><p>链接提取</p>

<p>一般爬虫都会有一些初始的种子URL，但是这些URL对于爬虫是远远不够的。爬虫在爬页面的时候，需要不断发现新的链接。</p></li>
<li><p>URL管理</p>

<p>最基础的URL管理，就是对已经爬过的URL和没有爬的URL做区分，防止重复爬取。</p></li>
<li><p>内容分析和持久化</p>

<p>一般来说，我们最终需要的都不是原始的HTML页面。我们需要对爬到的页面进行分析，转化成结构化的数据，并存储下来。</p></li>
</ul>


<p>不同的爬虫，对这几部分的要求是不一样的。</p>

<!--more-->


<p>对于通用型的爬虫，例如搜索引擎蜘蛛，需要指对互联网大部分网页无差别进行抓取。这时候难点就在于页面下载和链接管理上&mdash;如果要高效的抓取更多页面，就必须进行更快的下载；同时随着链接数量的增多，需要考虑如果对大规模的链接进行去重和调度，就成了一个很大的问题。一般这些问题都会在大公司有专门的团队去解决，比如这里有一篇来自淘宝的<a href="http://www.searchtb.com/2011/07/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BA%E5%AE%9E%E6%97%B6%E6%8A%93%E5%8F%96%E9%9B%86%E7%BE%A4.html?spm=0.0.0.0.hHzGxv">快速构建实时抓取集群</a>。</p>

<p>而垂直类型的爬虫要解决的问题则不一样，比如想要爬取一些网站的新闻、博客信息，一般抓取数量要求不是很大，难点则在于如何高效的定制一个爬虫，可以精确的抽取出网页的内容，并保存成结构化的数据。这方面需求很多，webmagic就是为了解决这个目的而开发的。</p>

<p>使用Java语言开发爬虫是比较复杂的。虽然Java有很强大的页面下载、HTML分析工具，但是每个都有不小的学习成本，而且这些工具本身都不是专门为爬虫而生，使用起来也没有那么顺手。我曾经有一年的时间都在开发爬虫，重复的开发让人头痛。Java还有一个比较成熟的框架<a href="https://code.google.com/p/crawler4j/"><strong>crawler4j</strong></a>，但是它是为通用爬虫而设计的，扩展性差一些，满足不了我的业务需要。我也有过自己开发框架的念头，但是终归觉得抽象的不是很好。直到发现python的爬虫框架<a href="http://scrapy.org/"><strong>scrapy</strong></a>，它将爬虫的生命周期拆分的非常清晰，我参照它进行了模块划分，并用Java的方式去实现了它，于是就有了webmagic。</p>

<p>代码已经托管到github，地址是<a href="https://github.com/code4craft/webmagic">https://github.com/code4craft/webmagic</a>，Javadoc：<a href="http://code4craft.github.io/webmagic/docs/">http://code4craft.github.io/webmagic/docs/</a></p>

<p>webmagic的实现还参考了另一个Java爬虫<a href="https://gitcafe.com/laiweiwei/Spiderman"><strong>SpiderMan</strong></a>。SpiderMan是一个全栈式的Java爬虫，它的设计思想跟webmagic稍有不同，它希望将Java语言的实现隔离，仅仅让用户通过配置就完成一个垂直爬虫。理论上，SpiderMan功能更强大，很多功能已经内置，而webmagic则比较灵活，适合熟悉Java语法的开发者，可以比较非常方便的进行扩展和二次开发。</p>

<hr />

<h2>webmagic的模块划分</h2>

<p>webmagic目前的核心代码都在<strong>webmagic-core</strong>中，<strong>webmagic-samples</strong>里有一些定制爬虫的例子，可以作为参考。而<strong>webmagic-plugin</strong>目前还不完善，后期准备加入一些并不太常用，但也有些用处的插件。下面主要介绍webmagic-core的内容。</p>

<p>前面说到，webmagic参考了scrapy的模块划分，分为Spider(整个爬虫的调度框架)、Downloader(页面下载)、PageProcessor(链接提取和页面分析)、Scheduler(URL管理)、Pipeline(离线分析和持久化)几部分。只不过scrapy通过middleware实现扩展，而webmagic则通过定义这几个接口，并将其不同的实现注入主框架类Spider来实现扩展。</p>

<p><img src="http://code4craft.github.io/images/posts/webmagic.png" alt="image" /></p>

<h3>Spider类-核心调度</h3>

<p>Spider是爬虫的入口类，Spider的接口调用采用了链式的API设计，其他功能全部通过接口注入Spider实现，下面是启动一个比较复杂的Spider的例子。</p>

<pre><code>Spider.create(sinaBlogProcessor)
.scheduler(new FileCacheQueueScheduler("/data/temp/webmagic/cache/"))
.pipeline(new FilePipeline())
.thread(10).run();
</code></pre>

<p>Spider的核心处理流程非常简单，代码如下：</p>

<pre><code>private void processRequest(Request request) {
    Page page = downloader.download(request, this);
    if (page == null) {
        sleep(site.getSleepTime());
        return;
    }
    pageProcessor.process(page);
    addRequest(page);
    for (Pipeline pipeline : pipelines) {
        pipeline.process(page, this);
    }
    sleep(site.getSleepTime());
}
</code></pre>

<h3>Downloader-页面下载</h3>

<p>页面下载是一切爬虫的开始。</p>

<p>大部分爬虫都是通过模拟http请求，接收并分析响应来完成。这方面，JDK自带的<strong>HttpURLConnection</strong>也可以满足最简单的需要，而<strong>Apache HttpClient</strong>(4.0后整合到HttpCompenent项目中)则是开发复杂爬虫的不二之选。它支持自定义HTTP头(对于爬虫比较有用的就是User-agent、cookie等)、自动redirect、连接复用、cookie保留、设置代理等诸多强大的功能。</p>

<p>webmagic使用了HttpClient 4.2，并封装到了<strong>HttpClientDownloader</strong>。学习HttpClient的使用对于构建高性能爬虫是非常有帮助的，官方的<a href="http://hc.apache.org/httpcomponents-client-ga/tutorial/html/">Tutorial</a>就是很好的学习资料。目前webmagic对HttpClient的使用仍在初步阶段，不过对于一般抓取任务，已经够用了。</p>

<p>下面是一个使用HttpClient最简单的例子：</p>

<pre><code>    HttpClient httpClient = new DefaultHttpClient();
    HttpGet httpGet = new HttpGet("http://youhost/xxx");
    HttpResponse httpResponse = httpClient.execute(httpGet);
    System.out.println(EntityUtils.toString(httpResponse.getEntity().getContent()));
</code></pre>

<p>webmagic还有一个好玩的功能<strong>FileDownloader</strong>。如果你不知道怎么分析页面，只是想先把它下载下来，那么可以使用FilePipeline将页面全文保存，下次运行的时候，使用FileDownloader，则可以直接从本地文件载入页面，并进行处理。对于其他模块，这跟实时下载并处理没有什么不同。</p>

<p>对于一些Javascript动态加载的网页，仅仅使用http模拟下载工具，并不能取到页面的内容。这方面的思路有两种：一种是抽丝剥茧，分析js的逻辑，再用爬虫去重现它(比如在网页中提取关键数据，再用这些数据去构造Ajax请求，最后直接从响应体获取想要的数据)；
另一种就是：内置一个浏览器，直接获取最后加载完的页面。这方面，js可以使用<strong>PhantomJS</strong>，它内部集成了webkit。而Java可以使用<strong>Selenium</strong>，这是一个非常强大的浏览器模拟工具。考虑以后将它整理成一个独立的Downloader，集成到webmagic中去。</p>

<p>一般没有必要去扩展Downloader。</p>

<h3>PageProcessor-页面分析及链接抽取</h3>

<p>这里说的页面分析主要指HTML页面的分析。页面分析可以说是垂直爬虫最复杂的一部分，在webmagic里，PageProcessor是定制爬虫的核心。</p>

<p>页面抽取最基本的方式是使用正则表达式。正则表达式好处是非常通用，解析文本的功能也很强大。但是正则表达式最大的问题是，不能真正对HTML进行语法级别的解析，没有办法处理关系到HTML结构的情况(例如处理标签嵌套)。例如，我想要抽取一个&lt;div>里的内容，可以这样写："&lt;div>(.*?)&lt;/div>&ldquo;。但是如果这个div内部还包含几个子div，这个时候使用正则表达式就会将子div的&rdquo;&lt;/div>&ldquo;作为终止符截取。为了解决这个问题，我们就需要进行HTML的分析。</p>

<p>HTML分析是一个比较复杂的工作，Java世界主要有几款比较方便的分析工具：</p>

<p><strong>Jsoup</strong></p>

<p><strong>HtmlParser</strong></p>

<p>这个项目很久没有维护了。我一直认为它目前还活的很好，很大程度上是它的名字太好了。</p>

<p><strong>Apache tika</strong></p>

<p>tika是专为抽取而生的工具，还支持PDF、Zip甚至是Java Class。使用tika分析HTML，需要自己定义一个抽取内容的Handler并继承<code>org.xml.sax.helpers.DefaultHandler</code>，解析方式就是xml标准的方式。crawler4j中就使用了tika作为解析工具。SAX这种流式的解析方式对于分析大文件很有用，我个人倒是认为对于解析html意义不是很大。</p>

<pre><code>InputStream inputStream = null;
    try {
      HtmlParser htmlParser = new HtmlParser();
        htmlParser.parse(new ByteArrayInputStream(page.getContentData()), contentHandler, metadata, new ParseContext());
    } catch (Exception e) {
        e.printStackTrace();
    }
</code></pre>

<p><strong>HtmlCleaner</strong></p>

<p>Selector是webmagic的模块中我最得意的部分，这里整合了xpath和正则表达式，并可以进行链式的抽取，很容易就实现强大的功能。例如，抽取一个页面某个区域的所有包含"blog"的链接，我可以这样写：</p>

<pre><code>page.getHtml().xpath("//div[@class='title']").links().regex(".*blog.*").toStrings();
</code></pre>

<p>这里用到了两个文本处理的DSL(Domain-specific language，领域特定语言)，一个是正则表达式，一个是xpath。</p>

<h3>Scheduler-URL管理</h3>

<p>URL管理的问题可大可小。对于小规模的抓取，URL管理是很简单的。我们只需要将待抓取URL和未抓取URL分开保存，并进行去重即可。使用JDK内置的集合类型Set、List或者Queue都可以满足需要。如果我们要进行多线程抓取，则可以选择线程安全的容器，例如LinkedBlockingQueue以及ConcurrentHashMap。</p>

<p>因为小规模的URL管理非常简单，很多框架都并不将其抽象为一个模块，而是直接融入到代码中。但是实际上，抽象出Scheduler模块，会使得框架的解耦程序上升一个档次，这也是我从scrapy中学到的。</p>

<p>在webmagic的设计中，除了Scheduler模块，其他的处理-从下载、解析到持久化，每个任务都是互相独立的，因此可以通过多个Spider共用一个Scheduler来进行扩展。排除去重的因素，URL管理天生就是一个队列，我们可以很方便的用分布式的队列工具去扩展它，也可以基于mysql、redis或者mongodb这样的存储工具来构造一个队列，这样构建一个多线程乃至分布式的爬虫就轻而易举了。</p>

<p>webmagic目前有两个Scheduler的实现，<strong>QueueScheduler</strong>是一个简单的内存队列，速度较快，并且是线程安全的，<strong>FileCacheQueueScheduler</strong>则是一个文件队列，它可以在较长的下载任务，中途停止后，下次执行仍然从中止的URL开始继续爬取。</p>

<h3>Pipeline-离线处理和持久化</h3>

<p>Pipeline其实也是比较有争议的一部分。大家都知道持久化的重要性，但是很多框架都选择直接在页面抽取的时候将持久化一起完成，例如crawer4j。但是Pipeline真正的好处是，将页面的在线分析和离线处理拆分开来，可以在一些线程里进行下载，另一些线程里进行处理和持久化。</p>

<p>你可以扩展Pipeline来实现抽取结果的持久化，将其保存到你想要保存的地方-本地文件、数据库、mongodb等等。Pipeline的处理目前还是在线的，但是修改为离线的也并不困难。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用webmagic抓取页面并保存为wordpress文件]]></title>
    <link href="http://code4craft.github.com/blog/2013/06/10/shi-yong-webmagiczhua-qu-ye-mian-bing-bao-cun-wei-wordpresswen-jian/"/>
    <updated>2013-06-10T18:05:00+08:00</updated>
    <id>http://code4craft.github.com/blog/2013/06/10/shi-yong-webmagiczhua-qu-ye-mian-bing-bao-cun-wei-wordpresswen-jian</id>
    <content type="html"><![CDATA[<p>之前做过一年的爬虫，当年功力不够，写的代码都是一点一点往上加。后来看了下据说是最优秀的爬虫<a href="http://www.oschina.net/p/scrapy"><code>scrapy</code></a>的结构，山寨了一个Java版的爬虫框架。</p>

<p>这个框架也分为Spider、Schedular、Downloader、Pipeline几个模块。此外有一个Selector，整合了常用的抽取技术(正则、xpath)，支持链式调用以及单复数切换，因为受够了各种抽取的正则，在抽取上多下了一点功夫。</p>

<!-- more -->


<p>废话不多，上代码。在webmagic里直接实现PageProcessor接口，即可实现一个爬虫。例如对我的点点博客<a href="http://progressdaily.diandian.com/">http://progressdaily.diandian.com/</a>进行抓取：</p>

<pre><code>    public class DiandianBlogProcessor implements PageProcessor {

        private Site site;

        @Override
        public void process(Page page) {
            //a()表示提取链接，as()表示提取所有链接
            //getHtml()返回Html对象，支持链式调用
            //r()表示用正则表达式提取一条内容，rs()表示提取多条内容
            //toString()表示取单条结果，toStrings()表示取多条
            List&lt;String&gt; requests = page.getHtml().as().rs("(.*/post/.*)").toStrings();
            //使用page.addTargetRequests()方法将待抓取的链接加入队列
            page.addTargetRequests(requests);
            //page.putField(key,value)将抽取的内容加入结果Map
            //x()和xs()使用xpath进行抽取
            page.putField("title", page.getHtml().x("//title").r("(.*?)\\|"));
            //sc()使用readability技术直接抽取正文，对于规整的文本有比较好的抽取正确率
            page.putField("content", page.getHtml().sc());
            page.putField("date", page.getUrl().r("post/(\\d+-\\d+-\\d+)/"));
            page.putField("id", page.getUrl().r("post/\\d+-\\d+-\\d+/(\\d+)"));
        }

        @Override
        public Site getSite() {
            //site定义抽取配置，以及开始url等
            if (site == null) {
                site = Site.me().setDomain("progressdaily.diandian.com").setStartUrl("http://progressdaily.diandian.com/").
                        setUserAgent("Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_2) AppleWebKit/537.31 (KHTML, like Gecko) Chrome/26.0.1410.65 Safari/537.31");
            }
            return site;
        }
    }
</code></pre>

<p>然后实现抓取代码：</p>

<pre><code>    public class DiandianProcessorTest {

        @Test
        public void test() throws IOException {
            DiandianBlogProcessor diandianBlogProcessor = new DiandianBlogProcessor();
            //pipeline是抓取结束后的处理
            //ftl文件放到classpath:ftl/文件夹下
            //默认放到/data/temp/webmagic/ftl/[domain]目录下
            FreemarkerPipeline pipeline = new FreemarkerPipeline("wordpress.ftl");
            //Spider.me()是简化写法，其实就是new一个啦
            //Spider.pipeline()设定一个pipeline，支持链式调用
            //ConsolePipeline输出结果到控制台
            //FileCacheQueueSchedular保存url，支持断点续传，临时文件输出到/data/temp/webmagic/cache目录
            //Spider.run()执行
            Spider.me().pipeline(new ConsolePipeline()).pipeline(pipeline).schedular(new FileCacheQueueSchedular(diaoyuwengProcessor.getSite(), "/data/temp/webmagic/cache/")).
                    processor(diaoyuwengProcessor).run();
        }
    }
</code></pre>

<p>跑一遍之后，将所有输出的文件，合并到一起，并加上wp的<a href="https://github.com/code4craft/webmagic/tree/master/webmagic-samples/src/main/resources">头尾</a>，就是wordpress-backup.xml了！</p>

<p>代码已开源<a href="https://github.com/code4craft/webmagic">https://github.com/code4craft/webmagic</a><strike>有什么邪恶用途你懂的…</strike></p>
]]></content>
  </entry>
  
</feed>
