<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[代码人生]]></title>
  <link href="http://code4craft.github.com/atom.xml" rel="self"/>
  <link href="http://code4craft.github.com/"/>
  <updated>2013-08-03T15:20:07+08:00</updated>
  <id>http://code4craft.github.com/</id>
  <author>
    <name><![CDATA[黄亿华]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[工作满两年时候的总结]]></title>
    <link href="http://code4craft.github.com/blog/2013/08/03/sum/"/>
    <updated>2013-08-03T15:15:00+08:00</updated>
    <id>http://code4craft.github.com/blog/2013/08/03/sum</id>
    <content type="html"><![CDATA[<p>最近工作处于低靡状态。公司玩转型，但是也没转出个啥效果。组里转到前端开发了，产品方向不定，至今也没做出个啥玩意来。工作内容缺少挑战性，更多是跟产品形态打交道了。不是很喜欢这个领域。升职的事也说不准，虽然自己只有两年工作经验，但是由于读书太久，岁数却接近30了，有点危机感。</p>

<p>生活上丰富多彩。女儿出生了，开始操心起来了。压力也大了些，要计划买房了。老婆工作不顺利，不过她自己也没太大所谓，随遇而安吧。</p>

<p>业余项目也挺丰富。BlackHoleJ和webmagic有了不少star了，都有了公司级用户，也得到了不少赞扬。</p>

<!--more-->


<p>webmagic反响出乎意料的好，还有用户去建了一个群，有个朋友基于这个做了一个企业级别的抓取工具(带管理后台的)，朋友们都戏称我教主了。能把这些东西分享出来，并得到肯定，这肯定是非常开心的。这个工具还有很多后续工作要做，最近为了它忙的头昏眼花的，孩子都有好多时候是老婆在带。所幸老婆非常认可我的业余爱好，但是自己多少也得学会安排时间了。</p>

<p>从毕业两年来，技术上的进步我自己是满意的，特别是还有能拿得出手的作品。今后的重点应该是深度为主。今年之后的计划是，好好把自己之前挖的几个坑：webmagic、MonkeySocks和taijicaptcha搞搞，都是有市场有挑战的东西。</p>

<p>另一方面，好好学学与人交往。必须要克服一下了，不然以后的职业发展不好走。</p>

<p>以一个健康的节奏生活并工作下去。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[webmagic的设计机制及原理-如何开发一个Java爬虫]]></title>
    <link href="http://code4craft.github.com/blog/2013/07/18/webmagicde-she-ji-ji-zhi-ji-yuan-li/"/>
    <updated>2013-07-18T14:52:00+08:00</updated>
    <id>http://code4craft.github.com/blog/2013/07/18/webmagicde-she-ji-ji-zhi-ji-yuan-li</id>
    <content type="html"><![CDATA[<p><img src="http://code4craft.github.io/images/posts/spider.jpeg" alt="image" /></p>

<p>之前就有网友在博客里留言，觉得webmagic的实现比较有意思，想要借此研究一下爬虫。最近终于集中精力，花了三天时间，终于写完了这篇文章。之前垂直爬虫写了一年多，webmagic框架写了一个多月，这方面倒是有一些心得，希望对读者有帮助。</p>

<h2>webmagic的目标</h2>

<p>一般来说，一个爬虫包括几个部分：</p>

<ul>
<li><p>页面下载</p>

<p> 页面下载是一个爬虫的基础。下载页面之后才能进行其他后续操作。</p></li>
<li><p>链接提取</p>

<p> 一般爬虫都会有一些初始的种子URL，但是这些URL对于爬虫是远远不够的。爬虫在爬页面的时候，需要不断发现新的链接。</p></li>
<li><p>URL管理</p>

<p> 最基础的URL管理，就是对已经爬过的URL和没有爬的URL做区分，防止重复爬取。</p></li>
<li><p>内容分析和持久化</p>

<p> 一般来说，我们最终需要的都不是原始的HTML页面。我们需要对爬到的页面进行分析，转化成结构化的数据，并存储下来。</p></li>
</ul>


<p>不同的爬虫，对这几部分的要求是不一样的。</p>

<!--more-->


<p>对于通用型的爬虫，例如搜索引擎蜘蛛，需要指对互联网大部分网页无差别进行抓取。这时候难点就在于页面下载和链接管理上&mdash;如果要高效的抓取更多页面，就必须进行更快的下载；同时随着链接数量的增多，需要考虑如果对大规模的链接进行去重和调度，就成了一个很大的问题。一般这些问题都会在大公司有专门的团队去解决，比如这里有一篇来自淘宝的<a href="http://www.searchtb.com/2011/07/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BA%E5%AE%9E%E6%97%B6%E6%8A%93%E5%8F%96%E9%9B%86%E7%BE%A4.html?spm=0.0.0.0.hHzGxv">快速构建实时抓取集群</a>。</p>

<p>而垂直类型的爬虫要解决的问题则不一样，比如想要爬取一些网站的新闻、博客信息，一般抓取数量要求不是很大，难点则在于如何高效的定制一个爬虫，可以精确的抽取出网页的内容，并保存成结构化的数据。这方面需求很多，webmagic就是为了解决这个目的而开发的。</p>

<p>使用Java语言开发爬虫是比较复杂的。虽然Java有很强大的页面下载、HTML分析工具，但是每个都有不小的学习成本，而且这些工具本身都不是专门为爬虫而生，使用起来也没有那么顺手。我曾经有一年的时间都在开发爬虫，重复的开发让人头痛。Java还有一个比较成熟的框架<a href="https://code.google.com/p/crawler4j/"><strong>crawler4j</strong></a>，但是它是为通用爬虫而设计的，扩展性差一些，满足不了我的业务需要。我也有过自己开发框架的念头，但是终归觉得抽象的不是很好。直到发现python的爬虫框架<a href="http://scrapy.org/"><strong>scrapy</strong></a>，它将爬虫的生命周期拆分的非常清晰，我参照它进行了模块划分，并用Java的方式去实现了它，于是就有了webmagic。</p>

<p>代码已经托管到github，地址是<a href="https://github.com/code4craft/webmagic">https://github.com/code4craft/webmagic</a>，Javadoc：<a href="http://code4craft.github.io/webmagic/docs/">http://code4craft.github.io/webmagic/docs/</a></p>

<p>webmagic的实现还参考了另一个Java爬虫<a href="https://gitcafe.com/laiweiwei/Spiderman"><strong>SpiderMan</strong></a>。SpiderMan是一个全栈式的Java爬虫，它的设计思想跟webmagic稍有不同，它希望将Java语言的实现隔离，仅仅让用户通过配置就完成一个垂直爬虫。理论上，SpiderMan功能更强大，很多功能已经内置，而webmagic则比较灵活，适合熟悉Java语法的开发者，可以比较非常方便的进行扩展和二次开发。</p>

<hr />

<h2>webmagic的模块划分</h2>

<p>webmagic目前的核心代码都在<strong>webmagic-core</strong>中，<strong>webmagic-samples</strong>里有一些定制爬虫的例子，可以作为参考。而<strong>webmagic-plugin</strong>目前还不完善，后期准备加入一些常用的功能。下面主要介绍webmagic-core的内容。</p>

<p>前面说到，webmagic参考了scrapy的模块划分，分为Spider(整个爬虫的调度框架)、Downloader(页面下载)、PageProcessor(链接提取和页面分析)、Scheduler(URL管理)、Pipeline(离线分析和持久化)几部分。只不过scrapy通过middleware实现扩展，而webmagic则通过定义这几个接口，并将其不同的实现注入主框架类Spider来实现扩展。</p>

<p><img src="http://code4craft.github.io/images/posts/webmagic-0.1.0.png" alt="image" /></p>

<h3>Spider类-核心调度</h3>

<p>Spider是爬虫的入口类，Spider的接口调用采用了链式的API设计，其他功能全部通过接口注入Spider实现，下面是启动一个比较复杂的Spider的例子。</p>

<figure class='code'><figcaption><span>启动一个Spider </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>     <span class="n">Spider</span><span class="o">.</span><span class="na">create</span><span class="o">(</span><span class="n">sinaBlogProcessor</span><span class="o">)</span>
</span><span class='line'>      <span class="o">.</span><span class="na">scheduler</span><span class="o">(</span><span class="k">new</span> <span class="n">FileCacheQueueScheduler</span><span class="o">(</span><span class="s">&quot;/data/temp/webmagic/cache/&quot;</span><span class="o">))</span>
</span><span class='line'>      <span class="o">.</span><span class="na">pipeline</span><span class="o">(</span><span class="k">new</span> <span class="n">FilePipeline</span><span class="o">())</span>
</span><span class='line'>      <span class="o">.</span><span class="na">thread</span><span class="o">(</span><span class="mi">10</span><span class="o">).</span><span class="na">run</span><span class="o">();</span>  
</span></code></pre></td></tr></table></div></figure>


<p>Spider的核心处理流程非常简单，代码如下：</p>

<figure class='code'><figcaption><span>Spider核心流程 </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>     <span class="kd">private</span> <span class="kt">void</span> <span class="nf">processRequest</span><span class="o">(</span><span class="n">Request</span> <span class="n">request</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>          <span class="n">Page</span> <span class="n">page</span> <span class="o">=</span> <span class="n">downloader</span><span class="o">.</span><span class="na">download</span><span class="o">(</span><span class="n">request</span><span class="o">,</span> <span class="k">this</span><span class="o">);</span>
</span><span class='line'>          <span class="k">if</span> <span class="o">(</span><span class="n">page</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>              <span class="n">sleep</span><span class="o">(</span><span class="n">site</span><span class="o">.</span><span class="na">getSleepTime</span><span class="o">());</span>
</span><span class='line'>              <span class="k">return</span><span class="o">;</span>
</span><span class='line'>          <span class="o">}</span>
</span><span class='line'>          <span class="n">pageProcessor</span><span class="o">.</span><span class="na">process</span><span class="o">(</span><span class="n">page</span><span class="o">);</span>
</span><span class='line'>          <span class="n">addRequest</span><span class="o">(</span><span class="n">page</span><span class="o">);</span>
</span><span class='line'>          <span class="k">for</span> <span class="o">(</span><span class="n">Pipeline</span> <span class="n">pipeline</span> <span class="o">:</span> <span class="n">pipelines</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>              <span class="n">pipeline</span><span class="o">.</span><span class="na">process</span><span class="o">(</span><span class="n">page</span><span class="o">,</span> <span class="k">this</span><span class="o">);</span>
</span><span class='line'>          <span class="o">}</span>
</span><span class='line'>          <span class="n">sleep</span><span class="o">(</span><span class="n">site</span><span class="o">.</span><span class="na">getSleepTime</span><span class="o">());</span>
</span><span class='line'>      <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Downloader-页面下载</h3>

<p>页面下载是一切爬虫的开始。</p>

<p>大部分爬虫都是通过模拟http请求，接收并分析响应来完成。这方面，JDK自带的<strong>HttpURLConnection</strong>可以满足最简单的需要，而<strong>Apache HttpClient</strong>(4.0后整合到HttpCompenent项目中)则是开发复杂爬虫的不二之选。它支持自定义HTTP头(对于爬虫比较有用的就是User-agent、cookie等)、自动redirect、连接复用、cookie保留、设置代理等诸多强大的功能。</p>

<p>webmagic使用了HttpClient 4.2，并封装到了<strong>HttpClientDownloader</strong>。学习HttpClient的使用对于构建高性能爬虫是非常有帮助的，官方的<a href="http://hc.apache.org/httpcomponents-client-ga/tutorial/html/">Tutorial</a>就是很好的学习资料。目前webmagic对HttpClient的使用仍在初步阶段，不过对于一般抓取任务，已经够用了。</p>

<p>下面是一个使用HttpClient最简单的例子：</p>

<figure class='code'><figcaption><span>HttpClient简单使用 </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>        <span class="n">HttpClient</span> <span class="n">httpClient</span> <span class="o">=</span> <span class="k">new</span> <span class="n">DefaultHttpClient</span><span class="o">();</span>
</span><span class='line'>        <span class="n">HttpGet</span> <span class="n">httpGet</span> <span class="o">=</span> <span class="k">new</span> <span class="n">HttpGet</span><span class="o">(</span><span class="s">&quot;http://youhost/xxx&quot;</span><span class="o">);</span>
</span><span class='line'>        <span class="n">HttpResponse</span> <span class="n">httpResponse</span> <span class="o">=</span> <span class="n">httpClient</span><span class="o">.</span><span class="na">execute</span><span class="o">(</span><span class="n">httpGet</span><span class="o">);</span>
</span><span class='line'>      <span class="n">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="n">EntityUtils</span><span class="o">.</span><span class="na">toString</span><span class="o">(</span><span class="n">httpResponse</span><span class="o">.</span><span class="na">getEntity</span><span class="o">().</span><span class="na">getContent</span><span class="o">()));</span>
</span></code></pre></td></tr></table></div></figure>


<p>对于一些Javascript动态加载的网页，仅仅使用http模拟下载工具，并不能取到页面的内容。这方面的思路有两种：一种是抽丝剥茧，分析js的逻辑，再用爬虫去重现它(比如在网页中提取关键数据，再用这些数据去构造Ajax请求，最后直接从响应体获取想要的数据)；
另一种就是：内置一个浏览器，直接获取最后加载完的页面。这方面，js可以使用<strong>PhantomJS</strong>，它内部集成了webkit。而Java可以使用<strong>Selenium</strong>，这是一个非常强大的浏览器模拟工具。考虑以后将它整理成一个独立的Downloader，集成到webmagic中去。</p>

<p>一般没有必要去扩展Downloader。</p>

<h3>PageProcessor-页面分析及链接抽取</h3>

<p>这里说的页面分析主要指HTML页面的分析。页面分析可以说是垂直爬虫最复杂的一部分，在webmagic里，PageProcessor是定制爬虫的核心。通过编写一个实现PageProcessor接口的类，就可以定制一个自己的爬虫。</p>

<p>页面抽取最基本的方式是使用正则表达式。正则表达式好处是非常通用，解析文本的功能也很强大。但是正则表达式最大的问题是，不能真正对HTML进行语法级别的解析，没有办法处理关系到HTML结构的情况(例如处理标签嵌套)。例如，我想要抽取一个&lt;div>里的内容，可以这样写：&#8221;&lt;div>(.*?)&lt;/div>&ldquo;。但是如果这个div内部还包含几个子div，这个时候使用正则表达式就会将子div的&rdquo;&lt;/div>&ldquo;作为终止符截取。为了解决这个问题，我们就需要进行HTML的分析。</p>

<p>HTML分析是一个比较复杂的工作，Java世界主要有几款比较方便的分析工具：</p>

<h4><strong>Jsoup</strong></h4>

<p>Jsoup是一个集强大和便利于一体的HTML解析工具。它方便的地方是，可以用于支持用jquery中css selector的方式选取元素，这对于熟悉js的开发者来说基本没有学习成本。</p>

<figure class='code'><figcaption><span>Jsoup的CSS Selector </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>     <span class="n">String</span> <span class="n">content</span> <span class="o">=</span> <span class="s">&quot;blabla&quot;</span><span class="o">;</span>
</span><span class='line'>      <span class="n">Document</span> <span class="n">doc</span> <span class="o">=</span> <span class="n">JSoup</span><span class="o">.</span><span class="na">parse</span><span class="o">(</span><span class="n">content</span><span class="o">);</span>
</span><span class='line'>      <span class="n">Elements</span> <span class="n">links</span> <span class="o">=</span> <span class="n">doc</span><span class="o">.</span><span class="na">select</span><span class="o">(</span><span class="s">&quot;a[href]&quot;</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>Jsoup还支持白名单过滤机制，对于网站防止XSS攻击也是很好的。</p>

<h4><strong>HtmlParser</strong></h4>

<p>HtmlParser的功能比较完备，也挺灵活，但谈不上方便。这个项目很久没有维护了，最新版本是2.1。HtmlParser的核心元素是Node，对应一个HTML标签，支持getChildren()等树状遍历方式。HtmlParser另外一个核心元素是NodeFilter，通过实现NodeFilter接口，可以对页面元素进行筛选。这里有一篇HtmlParser的使用文章：<a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-crawler/">使用 HttpClient 和 HtmlParser 实现简易爬虫</a>。</p>

<h4><strong>Apache tika</strong></h4>

<p>tika是专为抽取而生的工具，还支持PDF、Zip甚至是Java Class。使用tika分析HTML，需要自己定义一个抽取内容的Handler并继承<code>org.xml.sax.helpers.DefaultHandler</code>，解析方式就是xml标准的方式。crawler4j中就使用了tika作为解析工具。SAX这种流式的解析方式对于分析大文件很有用，我个人倒是认为对于解析html意义不是很大。</p>

<figure class='code'><figcaption><span>使用tika进行HTML解析 </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>     <span class="n">InputStream</span> <span class="n">inputStream</span> <span class="o">=</span> <span class="kc">null</span><span class="o">;</span>
</span><span class='line'>      <span class="n">HtmlParser</span> <span class="n">htmlParser</span> <span class="o">=</span> <span class="k">new</span> <span class="n">HtmlParser</span><span class="o">();</span>
</span><span class='line'>      <span class="n">htmlParser</span><span class="o">.</span><span class="na">parse</span><span class="o">(</span><span class="k">new</span> <span class="n">ByteArrayInputStream</span><span class="o">(</span><span class="n">page</span><span class="o">.</span><span class="na">getContentData</span><span class="o">()),</span>
</span><span class='line'>       <span class="n">contentHandler</span><span class="o">,</span> <span class="n">metadata</span><span class="o">,</span> <span class="k">new</span> <span class="n">ParseContext</span><span class="o">());</span>
</span></code></pre></td></tr></table></div></figure>


<h4><strong>HtmlCleaner与XPath</strong></h4>

<p>HtmlCleaner最大的优点是：支持XPath的方式选取元素。XPath是一门在XML中查找信息的语言，也可以用于抽取HTML元素。XPath与CSS Selector大部分功能都是重合的，但是CSS Selector专门针对HTML，写法更简洁，而XPath则是通用的标准，可以精确到属性值。XPath有一定的学习成本，但是对经常需要编写爬虫的人来说，这点投入绝对是值得的。</p>

<p>学习XPath可以参考w3school的<a href="http://www.w3school.com.cn/xpath/">XPath 教程</a>。下面是使用HtmlCleaner和xpath进行抽取的一段代码：</p>

<figure class='code'><figcaption><span>使用HtmlCleaner和XPath抽取元素 </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>     <span class="n">HtmlCleaner</span> <span class="n">htmlCleaner</span> <span class="o">=</span> <span class="k">new</span> <span class="n">HtmlCleaner</span><span class="o">();</span>
</span><span class='line'>      <span class="n">TagNode</span> <span class="n">tagNode</span> <span class="o">=</span> <span class="n">htmlCleaner</span><span class="o">.</span><span class="na">clean</span><span class="o">(</span><span class="n">text</span><span class="o">);</span>
</span><span class='line'>      <span class="n">Object</span><span class="o">[]</span> <span class="n">objects</span> <span class="o">=</span> <span class="n">tagNode</span><span class="o">.</span><span class="na">evaluateXPath</span><span class="o">(</span><span class="s">&quot;xpathStr&quot;</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<h4>几个工具的对比</h4>

<p>在这里评价这些工具的主要标准是“方便”。就拿抽取页面所有链接这一基本任务来说，几种代码分别如下：</p>

<p>XPath:</p>

<figure class='code'><figcaption><span>XPath提取链接 </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>     <span class="n">tagNode</span><span class="o">.</span><span class="na">evaluateXPath</span><span class="o">(</span><span class="s">&quot;//a/@href&quot;</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>CSS Selector:</p>

<figure class='code'><figcaption><span>CSS Selector提取链接 </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>     <span class="c1">//使用类似js的实现</span>
</span><span class='line'>      <span class="n">$</span><span class="o">(</span><span class="s">&quot;a[href]&quot;</span><span class="o">).</span><span class="na">attr</span><span class="o">(</span><span class="s">&quot;href&quot;</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>HtmlParser：</p>

<figure class='code'><figcaption><span>HtmlParser提取链接 </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>        <span class="n">Parser</span> <span class="n">p</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Parser</span><span class="o">(</span><span class="n">value</span><span class="o">);</span>
</span><span class='line'>        <span class="n">NodeFilter</span> <span class="n">aFilter</span> <span class="o">=</span> <span class="k">new</span> <span class="n">TagNameFilter</span><span class="o">(</span><span class="s">&quot;a&quot;</span><span class="o">);</span>
</span><span class='line'>        <span class="n">NodeList</span> <span class="n">nodes</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="na">extractAllNodesThatMatch</span><span class="o">(</span><span class="n">aFilter</span><span class="o">);</span>
</span><span class='line'>        <span class="k">for</span> <span class="o">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="o">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">nodes</span><span class="o">.</span><span class="na">size</span><span class="o">();</span> <span class="n">i</span><span class="o">++)</span> <span class="o">{</span>
</span><span class='line'>            <span class="n">Node</span> <span class="n">eachNode</span> <span class="o">=</span> <span class="n">nodes</span><span class="o">.</span><span class="na">elementAt</span><span class="o">(</span><span class="n">i</span><span class="o">);</span>
</span><span class='line'>            <span class="k">if</span> <span class="o">(</span><span class="n">eachNode</span> <span class="k">instanceof</span> <span class="n">LinkTag</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>                <span class="n">LinkTag</span> <span class="n">linkTag</span> <span class="o">=</span> <span class="o">(</span><span class="n">LinkTag</span><span class="o">)</span> <span class="n">eachNode</span><span class="o">;</span>
</span><span class='line'>                <span class="n">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="n">linkTag</span><span class="o">.</span><span class="na">extractLink</span><span class="o">());</span>
</span><span class='line'>            <span class="o">}</span>
</span><span class='line'>        <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>XPath是最简单的，可以精确选取到href属性值；而CSS Selector则次之，可以选取到HTML标签，属性值需要调用函数去获取；而HtmlParser和SAX则需要手动写程序去处理标签了，比较麻烦。</p>

<h4>webmagic的Selector</h4>

<p><strong>Selector</strong>是webmagic为了简化页面抽取开发的独立模块，是整个项目中我最得意的部分。这里整合了CSS Selector、XPath和正则表达式，并可以进行链式的抽取，很容易就实现强大的功能。即使你使用自己开发的爬虫工具，webmagic的Selector仍然值得一试。</p>

<p>例如，我已经下载了一个页面，现在要抽取某个区域的所有包含&#8221;blog&#8221;的链接，我可以这样写：</p>

<figure class='code'><figcaption><span>webmagic的链式抽取 </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>     <span class="c1">//content是用别的爬虫工具抽取到的正文</span>
</span><span class='line'>      <span class="n">String</span> <span class="n">content</span> <span class="o">=</span> <span class="s">&quot;blabla&quot;</span><span class="o">;</span>
</span><span class='line'>      <span class="n">List</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">links</span> <span class="o">=</span> <span class="n">Html</span><span class="o">.</span><span class="na">create</span><span class="o">(</span><span class="n">content</span><span class="o">)</span>
</span><span class='line'>      <span class="o">.</span><span class="n">$</span><span class="o">(</span><span class="s">&quot;div.title&quot;</span><span class="o">)</span>  <span class="c1">//css 选择，Java里虽然很少有$符号出现，不过貌似$作为方法名是合法的</span>
</span><span class='line'>      <span class="o">.</span><span class="na">xpath</span><span class="o">(</span><span class="s">&quot;//@href&quot;</span><span class="o">)</span>  <span class="c1">//提取链接</span>
</span><span class='line'>      <span class="o">.</span><span class="na">regex</span><span class="o">(</span><span class="s">&quot;.*blog.*&quot;</span><span class="o">)</span> <span class="c1">//正则匹配过滤</span>
</span><span class='line'>      <span class="o">.</span><span class="na">toStrings</span><span class="o">();</span> <span class="c1">//转换为string</span>
</span></code></pre></td></tr></table></div></figure>


<p>另外，webmagic的抓取链接需要显示的调用<code>Page.addTargetRequests()</code>去添加，这也是为了灵活性考虑的(很多时候，下一步的URL不是单纯的页面href链接，可能会根据页面模块进行抽取，甚至可能是自己拼凑出来的)。</p>

<p>补充一个有意思的话题，就是对于页面正文的自动抽取。相信用过Evernote Clearly都会对其自动抽取正文的技术印象深刻。这个技术又叫<strong>Readability</strong>，webmagic对readability有一个粗略的实现<strong>SmartContentSelector</strong>，用的是P标签密度计算的方法，在测试oschina博客时有不错的效果。</p>

<h3>Scheduler-URL管理</h3>

<p>URL管理的问题可大可小。对于小规模的抓取，URL管理是很简单的。我们只需要将待抓取URL和未抓取URL分开保存，并进行去重即可。使用JDK内置的集合类型Set、List或者Queue都可以满足需要。如果我们要进行多线程抓取，则可以选择线程安全的容器，例如LinkedBlockingQueue以及ConcurrentHashMap。</p>

<p>因为小规模的URL管理非常简单，很多框架都并不将其抽象为一个模块，而是直接融入到代码中。但是实际上，抽象出Scheduler模块，会使得框架的解耦程度上升一个档次，并非常容易进行横向扩展，这也是我从scrapy中学到的。</p>

<p>在webmagic的设计中，除了Scheduler模块，其他的处理-从下载、解析到持久化，每个任务都是互相独立的，因此可以通过多个Spider共用一个Scheduler来进行扩展。排除去重的因素，URL管理天生就是一个队列，我们可以很方便的用分布式的队列工具去扩展它，也可以基于mysql、redis或者mongodb这样的存储工具来构造一个队列，这样构建一个多线程乃至分布式的爬虫就轻而易举了。</p>

<p>URL去重也是一个比较复杂的问题。如果数据量较少，则使用hash的方式就能很好解决。数据量较大的情况下，可以使用Bloom Filter或者更复杂的方式。</p>

<p>webmagic目前有两个Scheduler的实现，<strong>QueueScheduler</strong>是一个简单的内存队列，速度较快，并且是线程安全的，<strong>FileCacheQueueScheduler</strong>则是一个文件队列，它可以用于耗时较长的下载任务，在任务中途停止后，下次执行仍然从中止的URL开始继续爬取。</p>

<h3>Pipeline-离线处理和持久化</h3>

<p>Pipeline其实也是容易被忽略的一部分。大家都知道持久化的重要性，但是很多框架都选择直接在页面抽取的时候将持久化一起完成，例如crawer4j。但是Pipeline真正的好处是，将页面的在线分析和离线处理拆分开来，可以在一些线程里进行下载，另一些线程里进行处理和持久化。</p>

<p>你可以扩展Pipeline来实现抽取结果的持久化，将其保存到你想要保存的地方-本地文件、数据库、mongodb等等。Pipeline的处理目前还是在线的，但是修改为离线的也并不困难。</p>

<p>webmagic目前只支持控制台输出和文件持久化，但是持久化到数据库也是很容易的。</p>

<h2>结语</h2>

<p>webmagic确实是一个山寨的框架，本身也没有太多创新的东西，但是确实对Java爬虫的实现有了一些简化。在强大便利的功能和较高的灵活性中间，webmagic选择了后者，目标就是要打造一个熟练的Java开发者也用的比较顺手的工具，并且可以集成到自己的业务系统中，这一点我自己开发了不少这样的业务，对其灵活性还是比较有信心的。webmagic目前的代码实现还比较简单(不到2000行)，如果有兴趣的阅读代码可能也会有一些收获，也非常欢迎建议和指正。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[monkeysocks架构规划]]></title>
    <link href="http://code4craft.github.com/blog/2013/07/07/monkeysocks-arch/"/>
    <updated>2013-07-07T21:35:00+08:00</updated>
    <id>http://code4craft.github.com/blog/2013/07/07/monkeysocks-arch</id>
    <content type="html"><![CDATA[<blockquote><p>monkeysocks的目标是为开发以及测试提供一个稳定的环境。它使用socks代理，将录制网络流量并本地保存，并在测试时将其重放。</p></blockquote>

<h2>jsocks的改造</h2>

<p>首先对公司一个项目进行了代理，测试结果：从开始启动到完成，只有4.7M的网络流量，本地空间开销不是问题。</p>

<p>今天把jsocks修改了下，将build工具换成了maven，并独立成了项目<a href="https://github.com/code4craft/monkeysocks/jsocks">https://github.com/code4craft/jsocks</a>。后来算是把record和replay功能做完了，开始研究各种协议replay的可能性。</p>

<!--more-->


<p>replay时候，如何知道哪个请求对应响应包是个大问题。开始的方式是把request报文的md5作为key，response作为value。</p>

<h2>TCP协议分析</h2>

<p>后来使用wiredshark结合程序日志来进行分析。</p>

<p>TCP协议栈大概是这样子：
<img src="http://www.skullbox.net/diagrams/tcppacket.gif?dur=673" alt="image" /></p>

<p>下面是wiredshark抓包的截图，从ea开始才是应用层协议的内容。</p>

<p><img src="http://code4craft.github.io/images/posts/tcp-wiredshark.png" alt="image" /></p>

<h2>应用层协议分析</h2>

<p>实现replay后，拿HTTP协议做了测试，自己用程序写了个URLConnection，倒是能够实现replay，但是换到浏览器里就很难了，因为cookie总是会有些不一样(现在基本上所有站点都会写cookie吧)。如果不对应用层协议本身进行分析，那么进行包的伪造就很难了。</p>

<p>https协议对于重放攻击做了处理，每次的请求包都不一样，也无法replay成功，暂时略过。</p>

<p>后来对于测试中得重点协议&mdash;mysql的协议，进行了研究。</p>

<p>这是一个有状态的协议，状态转移图如下：</p>

<p><img src="http://dev.mysql.com/doc/internals/en/images/graphviz-db6c3eaf9f35f362259756b257b670e75174c29b.png" alt="image" /></p>

<p>详细介绍<a href="http://dev.mysql.com/doc/internals/en/client-server-protocol.html">http://dev.mysql.com/doc/internals/en/client-server-protocol.html</a>，有点hold不住的感觉啊！</p>

<p>看了Authentication部分，会由server端发送一个随机数，来避免重放攻击。这个东西启发了我，因为主动权一般都是在server端，而我们要对client进行欺骗，难度就小了很多。</p>

<h2>架构设计</h2>

<p>后来决定把架构解耦了，fake server单独作为一个模块，可以单独启动成TCP server，也可以加入到jsocks里。最后架构是这样子：</p>

<p><img src="http://code4craft.github.io/images/posts/monkeysocks-arch.png" alt="image" /></p>

<p>fake servers的实现必定是个大坑，不过能把常用协议都了解一遍，本身也很有意思不是么？</p>

<h2>开发计划：</h2>

<ul>
<li><p>实现fake servers的TCP框架。</p></li>
<li><p>研究并实现常用协议的fake server。</p></li>
<li><p>确定持久化以及报文对应的策略。</p></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[monkeysocks开发日志]]></title>
    <link href="http://code4craft.github.com/blog/2013/07/06/monkeysocks/"/>
    <updated>2013-07-06T16:09:00+08:00</updated>
    <id>http://code4craft.github.com/blog/2013/07/06/monkeysocks</id>
    <content type="html"><![CDATA[<blockquote><p>monkeysocks的目标是为开发以及测试提供一个稳定的环境。</p></blockquote>

<h4>2013-7-5 动机</h4>

<p>前几天听说公司的测试团队在鼓捣数据固化的东西，说白了就是在测试启动时构建一个临时性的数据库，操作完之后再销毁，这样的好处是不造成测试副作用，同时屏蔽环境的差异。</p>

<!--more-->


<p>但是目前公司内部SOA用的太多了，仅仅靠数据库固化明显不现实，公司的架构团队做了一个将所有remote service放到本地启动的东西，但是这样子启动开销有点难以接受。有没有更可行的方案？</p>

<p>之前也有人做过一个单测的东西，可以将所有RPC调用的结果序列化成文本文件，下次调用时再序列化出来，这样其实就屏蔽了远程调用。但是Java语言层面的机制导致要把千奇八怪的对象序列化下来，本来就是不可完成的任务(有些对象本身就不是POJO，还有在getter、setter写逻辑的)。</p>

<p>于是我有一个大胆的设想：其实Java的外部依赖无非是网络IO，就是TCP/UDP包嘛，那我能不能做一个工具，录制一个稳定环境的网络流量，然后固化下来，最终在调用时进行重放，岂不是一劳永逸？</p>

<p>但是TCP/UDP毕竟是系统底层的东西，而且我想对每个Java进程单独做重放，所以只能从Java内部机制入手了。</p>

<p>有两个方法：</p>

<p>用cglib改写所有网络IO相关的接口，改用固化调用。</p>

<p>设置Java全局socks代理，并启动socks server，在socks server里做代理。</p>

<p>显然第二种方法更简单，有四两拨千斤的感觉！</p>

<p>找到一个Java socks server，jsocks，最初版本比较老，google code上有一个改进版，用的是ant，因为以后要集成肯定要用maven，于是就做了点maven化的处理，考虑以后单独做成一个项目，现在先改了测试下可行性吧。<a href="https://github.com/code4craft/monkeysocks/jsocks">https://github.com/code4craft/jsocks</a></p>

<p>Java里面设置全局socksProxy的方法见<a href="http://docs.oracle.com/javase/6/docs/technotes/guides/net/proxies.html">http://docs.oracle.com/javase/6/docs/technotes/guides/net/proxies.html</a>。</p>

<p>鼓捣一下，成功启动起来，明天先对公司的项目进行试用。</p>

<h4>2013-7-6</h4>

<p>今天开始了对socks的摸索。</p>

<p>首先对公司一个项目进行了代理，测试结果：从开始启动到完成，只有4.7M的网络流量，本地空间开销不是问题。</p>

<p>想到咱这个不就是个TCP重放攻击么？了解了一下一些协议防重放攻击的机制，发现大多是在server端做，那么其实client端的请求并无不同，希望是这样！</p>

<p>研究了一下http协议，response竟然有date项，希望不会作为判断依据，要不然还要做http解析，那就纠结了！怎么觉得自己老是在研究怎么实现一个gfw呢？</p>

<p>在测试中，遇到了问题：</p>

<p>很多协议里都自带了版本号，比如<a href="http://www.hoterran.info/mysql-protocol-soucecode-2">mysql</a>、zookeeper，这样就给识别请求和伪造响应带来了难度。幸好公司内部用的工具不是太多，理论上还是在可控状态。</p>

<p>最终决定结构大概是这样子：</p>

<p><img src="http://code4craft.github.io/images/posts/mocksocks-flow-in.png" alt="image" /></p>

<p><img src="http://code4craft.github.io/images/posts/mocksocks-flow-out.png" alt="image" /></p>

<p>晚上尝试了一下，jsocks的流程写的过于凌乱，最终缓存结构也没定好，不说了，碎觉！</p>

<h4>2013-7-7</h4>

<p>鼓捣了下架构的事情，文章单独整理了下，链接：<a href="http://code4craft.github.io/blog/2013/07/07/monkeysocks-arch/">http://code4craft.github.io/blog/2013/07/06/monkeysocks-arch/</a></p>

<h4>2013-7-8</h4>

<p>今天跟水哥讨论起两个问题，假设拿到一个报文byte[]，存在两个难点，一个是对于可变部分的判断和伪造，另一个是对于包结构的近似匹配。</p>

<p>可变部分的的判断，水哥说可以用录两次，去掉不同的部分，就变成一个不变的基和一个过滤器，然后对于以后的包，就用过滤器过滤后找到这个基。</p>

<p>包结构的近似匹配，水哥提议对包的所有字节进行求和，过滤掉位置信息，似乎也是可行的方向？</p>

<p>用Java实现一个流管道：</p>

<p><a href="http://ostermiller.org/convert_java_outputstream_inputstream.html">http://ostermiller.org/convert_java_outputstream_inputstream.html</a></p>

<h4>2013-7-9</h4>

<p>使用byte和条件变量实现了一个Java流的管道，线程安全的，debug了好久终于成功，当年操作系统的知识还是没还给老师！<a href="https://github.com/code4craft/monkeysocks/blob/master/monkeysocks-server/src/main/java/com/dianping/monkeysocks/socket/StreamBuffer.java">https://github.com/code4craft/monkeysocks/blob/master/monkeysocks-server/src/main/java/com/dianping/monkeysocks/socket/StreamBuffer.java</a></p>

<p>这个东东使得伪造一个Socket成为了可能，下面就是使用Socket API实现fake servers的逻辑了。</p>

<h4>2013-7-11</h4>

<p>昨天太累了，项目又比较忙，先休息一天。</p>

<p>今天早上考虑用自己写的TCP server框架把jsocks的连接部分重写一遍(那堆代码全部在一个类中，实在是太复杂了)，想到
今天早上想了下InputStream这些玩意，觉得这样总会占用一个线程去把Stream转来转去的，弄成onReceive()这样的异步调用的话，就很容易做到链式调用了。</p>

<p>这东西太费脑子了，先休息一天半天再说。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[玩转github之--神啊满足我的虚荣心吧]]></title>
    <link href="http://code4craft.github.com/blog/2013/07/03/show-in-github/"/>
    <updated>2013-07-03T07:47:00+08:00</updated>
    <id>http://code4craft.github.com/blog/2013/07/03/show-in-github</id>
    <content type="html"><![CDATA[<h3>github版简历</h3>

<p><a href="http://resume.github.io/">http://resume.github.io/</a>上有这个东东，但是样式太难看了。看到一个挺不错的模板<a href="https://github.com/hit9/GhResume">https://github.com/hit9/GhResume</a>，就给用上了。我的简历：
<a href="http://code4craft.github.io/GhResume/">http://code4craft.github.io/GhResume/</a></p>

<h3>关注star</h3>

<p>最近做的项目在github每天会有几个star，出于虚荣心嘛，经常忍不住就会去看看，有人star了没？有人fork了没？</p>

<p>每天看太麻烦了，干脆做成一个chrome插件，带桌面通知，有新star提醒，岂不开心？</p>

<p>于是有了<a href="https://github.com/code4craft/exciting"><strong>exciting</strong></a>！ 哥也会做chrome插件了！液！</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[一个shell下的todolist]]></title>
    <link href="http://code4craft.github.com/blog/2013/06/28/todolist/"/>
    <updated>2013-06-28T10:17:00+08:00</updated>
    <id>http://code4craft.github.com/blog/2013/06/28/todolist</id>
    <content type="html"><![CDATA[<p>使用和文件保存都挺简单，试试看吧！</p>

<p><a href="http://todotxt.com/">http://todotxt.com/</a>貌似还能同步到手机端？</p>

<p>下载之后，</p>

<pre><code>ln -s xxx/todo.sh /usr/local/bin/todo
</code></pre>

<p>貌似这样会把东西存到/usr/local/bin/todo.txt里？不管了！</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[写了个快捷保存文本的shell工具]]></title>
    <link href="http://code4craft.github.com/blog/2013/06/27/yong-pythonxie-liao-ge-xiao-gong-ju/"/>
    <updated>2013-06-27T22:14:00+08:00</updated>
    <id>http://code4craft.github.com/blog/2013/06/27/yong-pythonxie-liao-ge-xiao-gong-ju</id>
    <content type="html"><![CDATA[<p>作为一个Java程序员，对脚本语言程序员自己随时写些小工具，已经羡慕很久了，于是最终开始了脚本语言的学习。</p>

<!-- more -->


<p>第一个工具是个助记程序。像工作中我们总会管理一些文本，比如内网服务器地址、git仓库地址什么的，之前一直都是要登录一个平台去拷贝下来，然后粘贴到shell里，特别麻烦。想写了个工具，为一个文本写一个助记符，然后保存到文件里，每次可以调用程序把文件读出来，再根据助记符查找到对应文本。用法可以这样：</p>

<pre><code>add alias text
get alias
</code></pre>

<p>例如：</p>

<pre><code>add getter https://github.com/code4craft/getter 
</code></pre>

<p>也可以用&#8220;符号穿插到程序里：</p>

<pre><code>git clone `get getter`
</code></pre>

<p>刚开始用python写了一遍，把文件保存到~/.getrc，并每次读出来并更新。</p>

<p>后来想了想，这个方法没有自动补全，太麻烦！有没有好办法？最终想到，干脆把name作为可执行文件名，value作为程序的输出，每个name对应一个输出，再以特定前缀开始，这样子也有自动提示了，岂不是更好？于是就有了shell版本：</p>

<pre><code>add.sh

 #!/bin/sh
FILE_PATH=/usr/local/getter
PREFIX=-
[ -d "$FILE_PATH" ] || mkdir -p $FILE_PATH
if [ -z "$1" ]
then
    echo "Usage: $0 alias [text]"
else
    FILE_NAME=$FILE_PATH/$PREFIX$1
    if [ -z "$2" ]
    then
        rm -f $FILE_NAME
    else
        echo "#!/bin/sh" &gt; $FILE_NAME
        echo "echo \"$2\"" &gt;&gt; $FILE_NAME
        chmod +x $FILE_NAME
    fi
fi
</code></pre>

<p>记得在/etc/bashrc中加一行</p>

<pre><code>export PATH=$PATH:/usr/local/getter
</code></pre>

<p>后来一想，那么如果我要保存一个可执行的shell命令，执行时是不是要<code>-xx</code>?干脆把命令直接写到脚本里，也别echo了！</p>

<pre><code>edd.sh

 #!/bin/sh
FILE_PATH=/usr/local/getter
PREFIX=-
[ -d "$FILE_PATH" ] || mkdir -p $FILE_PATH
if [ -z "$1" ]
then
    echo "Usage: $0 alias [command]"
else
    FILE_NAME=$FILE_PATH/$PREFIX$1
    if [ -z "$2" ]
    then
        rm -f $FILE_NAME
    else
        echo "#!/bin/sh" &gt; $FILE_NAME
        echo "$2 \$1 \$2 \$3" &gt;&gt; $FILE_NAME
        chmod +x $FILE_NAME
    fi
fi
</code></pre>

<p>比如之后我要保存一条命令&#8221;curl <a href="http://code4craft.github.io/blackhole/install.sh">http://code4craft.github.io/blackhole/install.sh</a> | sh&#8221;，可以这么用：</p>

<pre><code>#保存命令
edd install "curl http://code4craft.github.io/blackhole/install.sh | sh"
#执行命令
-install
</code></pre>

<p>代码已经放到了github:</p>

<p><a href="https://github.com/code4craft/getter">https://github.com/code4craft/getter</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[BlackHole开发日记-一次压力测试及JVM调优的经过]]></title>
    <link href="http://code4craft.github.com/blog/2013/06/23/%5B%3F%5D-ci-jvmdiao-you-de-jing-guo/"/>
    <updated>2013-06-23T06:44:00+08:00</updated>
    <id>http://code4craft.github.com/blog/2013/06/23/[?]-ci-jvmdiao-you-de-jing-guo</id>
    <content type="html"><![CDATA[<p>BlackHole开发很久了，目前稳定性、性能都还可以了，但是作为一个Java程序，内存开销一直是硬伤，动不动100M内存下去了，对于单机用户实在是不太友好。</p>

<p>怎么办？优化先从分析开始！</p>

<h3>获取内存信息</h3>

<p>获取内存信息一般使用jmap。</p>

<pre><code>jmap -histo pid
</code></pre>

<p>这种方式获取到得比较简略，我们可以先把内存dump下来，再进行离线分析。jhat是一个离线内存分析工具，会开启一个web服务以供展示。</p>

<pre><code>jmap -dump:file=dumpfile pid
jhat -J-Xmx512m dumpfile
</code></pre>

<!-- more -->


<p>访问<a href="http://127.0.0.1:7000%E5%8D%B3%E5%8F%AF%E3%80%82%E9%BB%98%E8%AE%A4%E6%98%AFClass%E5%88%97%E8%A1%A8%EF%BC%8C%E7%BF%BB%E5%88%B0%E9%A1%B5%E9%9D%A2%E5%BA%95%E9%83%A8%E5%8F%AF%E4%BB%A5%E6%9F%A5%E7%9C%8B%E5%85%B6%E4%BB%96%E5%8A%9F%E8%83%BD%E3%80%82">http://127.0.0.1:7000%E5%8D%B3%E5%8F%AF%E3%80%82%E9%BB%98%E8%AE%A4%E6%98%AFClass%E5%88%97%E8%A1%A8%EF%BC%8C%E7%BF%BB%E5%88%B0%E9%A1%B5%E9%9D%A2%E5%BA%95%E9%83%A8%E5%8F%AF%E4%BB%A5%E6%9F%A5%E7%9C%8B%E5%85%B6%E4%BB%96%E5%8A%9F%E8%83%BD%E3%80%82</a></p>

<p>参考资料：</p>

<p><a href="http://www.lhelper.org/newblog/?tag=jhat">http://www.lhelper.org/newblog/?tag=jhat</a>
<a href="http://blog.csdn.net/gtuu0123/article/details/6039474">http://blog.csdn.net/gtuu0123/article/details/6039474</a></p>

<h3>详细分析</h3>

<p>以下分析仅针对JDK 7 HotSpot虚拟机。jmap -histo的结果：</p>

<pre><code>num     #instances         #bytes  class name
----------------------------------------------
 1:         28490        4057072  &lt;constMethodKlass&gt;
 2:         28490        3882464  &lt;methodKlass&gt;
 3:          2630        2820064  &lt;constantPoolKlass&gt;
 4:         46350        2412600  &lt;symbolKlass&gt;
 5:         32778        2372824  [C
 6:          2630        1990744  &lt;instanceKlassKlass&gt;
 7:          3418        1955208  [I
 8:         15491        1911568  [B
 9:          2347        1845400  &lt;constantPoolCacheKlass&gt;
10:         19246         615872  java.lang.String
11:           256         561152  [Lnet.sf.ehcache.store.chm.SelectableConcurrentHashMap$HashEntry;
12:          2930         304720  java.lang.Class
13:          3740         247280  [S
14:          4321         221520  [[I
</code></pre>

<p>其中[开头表示数组，[C [I [B 分别是char[] int[] byte[]。</p>

<p>constMethodKlass、都实现自sun.jvm.hotspot.oops.Klass，用于在永久代里保存类的信息。</p>

<p>换到JDK6之后，发现永久代的消耗下去了。</p>

<pre><code>num     #instances         #bytes  class name
----------------------------------------------
 1:         10823        3072312  [B
 2:         16605        2318720  &lt;constMethodKlass&gt;
 3:         18687        1388088  [C
 4:         16605        1328608  &lt;methodKlass&gt;
 5:         27595        1296832  &lt;symbolKlass&gt;
 6:          1699         940392  &lt;constantPoolKlass&gt;
 7:          2520         883408  [I
 8:          1699         724944  &lt;instanceKlassKlass&gt;
 9:          1472         565136  &lt;constantPoolCacheKlass&gt;
10:           256         561152  [Lnet.sf.ehcache.store.chm.SelectableConcurrentHashMap$HashEntry;
11:         12148         291552  java.lang.String
12:          4505         288320  net.sf.ehcache.Element
13:          7290         233280  java.lang.ThreadLocal$ThreadLocalMap$Entry
14:          1946         186816  java.lang.Class
15:          4509         180360  net.sf.ehcache.store.chm.SelectableConcurrentHashMap$HashEntry
</code></pre>

<p>查看一下总的内存开销(参考资料:
<a href="http://yytian.blog.51cto.com/535845/574527">http://yytian.blog.51cto.com/535845/574527</a>)</p>

<pre><code>ps -e -o 'pid,comm,args,pcpu,rsz' | grep java |  sort -nrk5
1239 java            java -jar -Djava.io.tmpdir=  0.1 52816
</code></pre>

<p>或者：</p>

<pre><code>top -pid pid
</code></pre>

<p>查看到只有52m，看来JDK7占用内存果然增加了！</p>

<h3>压力测试</h3>

<p>使用140,000个随机域名做压力测试。发现之前使用的JDK7 Developer Preview u04，在短时间产生大量对象的时候，GC会失去作用，内存迅速飙升到200M，后来更新到1.7.0u25，稳定到了130m，qps大概在49000~50000之间。</p>

<p>尝试使用ConcurrentHashMap代替EhCache，qps提高到50000~51000之间，变化不明显，EhCache还是相当优秀的。</p>

<p>为了提高吞吐量，查看GC情况:</p>

<pre><code>sudo jstat -gcutil 2960
S0     S1     E      O      P     YGC     YGCT    FGC    FGCT     GCT   
0.00   0.75  30.35  12.06  72.92     25    0.063     0    0.000    0.063
</code></pre>

<p>140,000个请求产生了25次YGC。</p>

<p>参考了关于JVM的调优文章<a href="http://blog.csdn.net/kthq/article/details/8618052">http://blog.csdn.net/kthq/article/details/8618052</a></p>

<p>重新设置新生代为200m，并开启并行收集：</p>

<pre><code>JVM_OPTION="-XX:+UseParallelGC -XX:NewSize=200m"
</code></pre>

<p>140,000个请求只产生了3次YGC，但是qps变化并不明显。看来新生代设置大之后，虽然YGC少了，但是一次回收的时间多了，最终其实没啥影响啊，那还是弄小一点好了。</p>

<p>看来折腾JVM效果不明显，除了内存开销之外，其他没有明显变化。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[换电脑了]]></title>
    <link href="http://code4craft.github.com/blog/2013/06/22/huan-dian-nao-liao/"/>
    <updated>2013-06-22T08:57:00+08:00</updated>
    <id>http://code4craft.github.com/blog/2013/06/22/huan-dian-nao-liao</id>
    <content type="html"><![CDATA[<p>公司政策，每个人给6500软妹币报销，可以买电脑。因为刚好WWDC开完，老AIR降价，本着好用和省钱的标准，就买了老AIR一台，7388包发票。</p>

<p>换到AIR最不习惯的是屏幕，PRO的屏幕是1280*800，AIR的屏幕是1440x900，所以字总是特小，原来12pt的字体非要14pt才能显示出来，MAC还没法设置默认字体，只能装了个Tinker Tool。设置之后，各种丑和切边…没办法，办公用，眼睛不累是原则！</p>

<p>换电脑之后，得把老的东西弄过来。这次RVM倒是一次成功了，换了gem的source为淘宝的，更新还挺快的。octpress又重新装了一次，好像ruby的包好多都是安装到一个目录下可用的？最后只能把原来的octpress连同博客一起拷贝过来了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用webmagic抓取页面并保存为wordpress文件]]></title>
    <link href="http://code4craft.github.com/blog/2013/06/10/shi-yong-webmagiczhua-qu-ye-mian-bing-bao-cun-wei-wordpresswen-jian/"/>
    <updated>2013-06-10T18:05:00+08:00</updated>
    <id>http://code4craft.github.com/blog/2013/06/10/shi-yong-webmagiczhua-qu-ye-mian-bing-bao-cun-wei-wordpresswen-jian</id>
    <content type="html"><![CDATA[<p>之前做过一年的爬虫，当年功力不够，写的代码都是一点一点往上加。后来看了下据说是最优秀的爬虫<a href="http://www.oschina.net/p/scrapy"><code>scrapy</code></a>的结构，山寨了一个Java版的爬虫框架。</p>

<p>这个框架也分为Spider、Schedular、Downloader、Pipeline几个模块。此外有一个Selector，整合了常用的抽取技术(正则、xpath)，支持链式调用以及单复数切换，因为受够了各种抽取的正则，在抽取上多下了一点功夫。</p>

<!-- more -->


<p>废话不多，上代码。在webmagic里直接实现PageProcessor接口，即可实现一个爬虫。例如对我的点点博客<a href="http://progressdaily.diandian.com/">http://progressdaily.diandian.com/</a>进行抓取：</p>

<pre><code>    public class DiandianBlogProcessor implements PageProcessor {

        private Site site;

        @Override
        public void process(Page page) {
            //a()表示提取链接，as()表示提取所有链接
            //getHtml()返回Html对象，支持链式调用
            //r()表示用正则表达式提取一条内容，rs()表示提取多条内容
            //toString()表示取单条结果，toStrings()表示取多条
            List&lt;String&gt; requests = page.getHtml().as().rs("(.*/post/.*)").toStrings();
            //使用page.addTargetRequests()方法将待抓取的链接加入队列
            page.addTargetRequests(requests);
            //page.putField(key,value)将抽取的内容加入结果Map
            //x()和xs()使用xpath进行抽取
            page.putField("title", page.getHtml().x("//title").r("(.*?)\\|"));
            //sc()使用readability技术直接抽取正文，对于规整的文本有比较好的抽取正确率
            page.putField("content", page.getHtml().sc());
            page.putField("date", page.getUrl().r("post/(\\d+-\\d+-\\d+)/"));
            page.putField("id", page.getUrl().r("post/\\d+-\\d+-\\d+/(\\d+)"));
        }

        @Override
        public Site getSite() {
            //site定义抽取配置，以及开始url等
            if (site == null) {
                site = Site.me().setDomain("progressdaily.diandian.com").setStartUrl("http://progressdaily.diandian.com/").
                        setUserAgent("Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_2) AppleWebKit/537.31 (KHTML, like Gecko) Chrome/26.0.1410.65 Safari/537.31");
            }
            return site;
        }
    }
</code></pre>

<p>然后实现抓取代码：</p>

<pre><code>    public class DiandianProcessorTest {

        @Test
        public void test() throws IOException {
            DiandianBlogProcessor diandianBlogProcessor = new DiandianBlogProcessor();
            //pipeline是抓取结束后的处理
            //ftl文件放到classpath:ftl/文件夹下
            //默认放到/data/temp/webmagic/ftl/[domain]目录下
            FreemarkerPipeline pipeline = new FreemarkerPipeline("wordpress.ftl");
            //Spider.me()是简化写法，其实就是new一个啦
            //Spider.pipeline()设定一个pipeline，支持链式调用
            //ConsolePipeline输出结果到控制台
            //FileCacheQueueSchedular保存url，支持断点续传，临时文件输出到/data/temp/webmagic/cache目录
            //Spider.run()执行
            Spider.me().pipeline(new ConsolePipeline()).pipeline(pipeline).schedular(new FileCacheQueueSchedular(diaoyuwengProcessor.getSite(), "/data/temp/webmagic/cache/")).
                    processor(diaoyuwengProcessor).run();
        }
    }
</code></pre>

<p>跑一遍之后，将所有输出的文件，合并到一起，并加上wp的<a href="https://github.com/code4craft/webmagic/tree/master/webmagic-samples/src/main/resources">头尾</a>，就是wordpress-backup.xml了！</p>

<p>代码已开源<a href="https://github.com/code4craft/webmagic">https://github.com/code4craft/webmagic</a><strike>有什么邪恶用途你懂的…</strike></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[2013年半年总结及计划]]></title>
    <link href="http://code4craft.github.com/blog/2013/06/07/2013nian-ban-nian-zong-jie-ji-ji-hua/"/>
    <updated>2013-06-07T07:08:00+08:00</updated>
    <id>http://code4craft.github.com/blog/2013/06/07/2013nian-ban-nian-zong-jie-ji-ji-hua</id>
    <content type="html"><![CDATA[<p>2013年前半年搞Scrum，工作忙了不少，但是却是瞎忙的多。</p>

<p>持续维护<code>BlackHole</code>半年时间，github上有了 <strong>24</strong> 个star，半年来最大的成果。</p>

<!-- more -->


<p>看了 <strong> 3 </strong> 本书，比较少：</p>

<p>《git权威指南》、《硝烟中的Scrum和XP》、《七周七语言》</p>

<p>又写了两个项目：</p>

<p><code>webmagic</code>(爬虫框架)和<code>hostd</code>(BlackHole衍生管理工具)</p>

<p>阅读代码：</p>

<p>读了公司的JMS框架swallow、RPC框架Pigeon。</p>

<p>尝试阅读Jetty源码，未果。</p>

<p>学些了一门新语言：</p>

<p><code>Javascript</code></p>

<p>写了20篇技术博客，有一篇上了oschina首页。</p>

<p>在微博上做了个<code>每天学点新工具</code>系列，发了 <strong> 7 </strong> 条微博。</p>

<p>量上有所下降，特别是读书一项。</p>

<p>玩新东西多了，钻研少了。</p>

<p>下半年的计划是找一个Web服务器进行深入研究。</p>

<p>Jetty、Tomcat、Netty、nginx选一个吧。</p>

<p>Jetty最简单，看到一半了，学习的应该是架构、设计的东西。</p>

<p>Tomcat比较复杂，据说代码可读性也不太好，但是毕竟是目前使用的容器，可以试试。</p>

<p>Netty，学好这个，做个中间件是信手拈来的事，这方面需求挺多的。难处是用的比较少，难以把握其原理。</p>

<p>nginx源码挑战最大，但是收获应该最多，进入c世界？BlackHole自认为也是一个高性能服务器了，跟nginx比比，这样会对高性能Web服务有个全面的认识。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[IntelliJ使用心得]]></title>
    <link href="http://code4craft.github.com/blog/2013/04/17/intellijshi-yong-xin-de/"/>
    <updated>2013-04-17T21:18:00+08:00</updated>
    <id>http://code4craft.github.com/blog/2013/04/17/intellijshi-yong-xin-de</id>
    <content type="html"><![CDATA[<p>最近尝鲜试用了一下IntelliJ，使用下来还是比较爽的，最后我这个很少花钱买软件的人，也在oschina上买了个人版。IDE毕竟是码农干活的家伙，想想也值了。使用的时候有一些心得，记录下来。</p>

<!-- more -->


<h3>调整界面为酷酷的黑色</h3>

<p>Preferences=>Appearance=>theme=>Darcula</p>

<h3>检出项目:</h3>

<p>VCS=>Checkout From Version Control，maven项目会被自动识别出来。</p>

<h3>设置快捷键：</h3>

<p>Preferences=>keymaps，有很多套方案，当然即使选择Eclipse也还是有很多和Eclipse不同的地方。</p>

<h3>自动补全：</h3>

<p>Mac下默认是clt+space，可以使用keymaps=>Main menu=>Code=>Competion设置。</p>

<h3>去除自动补全的大小写敏感：</h3>

<p>不知道多少童鞋和我一样被Eclipse惯坏了，使用自动补全完全不注意大小写的，IntelliJ默认区分大小写，很是让人难过。不过在Editor=>Code Completion里把Case sensitive completion设置为None就可以了。</p>

<h3>自动展开目录</h3>

<p>Eclipse有个打开文件就自动展开目录的功能，在IntelliJ里从Project左边栏的齿轮上选择Autoscroll to Source和Autoscroll from Source都勾选上即可。</p>

<h3>使用Tomcat运行web项目：</h3>

<p>需安装插件：Tomcat and TomEE intergration</p>

<p>选择Run=>Edit Configurations，点+，选tomcat server，Deloyment选择对应artifact。详细文章：<a href="http://my.oschina.net/tsl0922/blog/94621">http://my.oschina.net/tsl0922/blog/94621</a></p>

<h3>项目间文件复制</h3>

<p>IntelliJ里的工作空间是Project，不同Project之间是没有什么关系的。在一个Project里copy&amp;paste，会弹出对话框，让你选择<strong>目标文件夹</strong>。也就是说，并没有跨Project的复制，而是从源Project把文件复制出去。</p>

<h3>自动编译</h3>

<p>IntelliJ默认是不会自动编译项目的，所以在run之前会有个make的过程，习惯自动编译项目的可以在这里打开：Compiler=>make project automatically。因为IntelliJ项目空间不大，所以开启之后也不会像Eclipse一样出现build workspace很久的情况。</p>

<h3>Debug</h3>

<p>debug最好不要使用method breakpoint，会导致启动异常缓慢，博主之前就不小心启动了method breakpoint，然后进入调试要花掉几分钟的时间。IntelliJ断点可以设置Condition，其实Eclipse也可以，只不过没有这么明显，同时IntelliJ可以在Condition进行代码提示。</p>

<h3>File Template</h3>

<p>与Eclipse的Code Template类似，只不过IntelliJ内置变量全部为大写，例如：${NAME}。可以使用#parse(&ldquo;File Header.java&rdquo;)这种格式来导入另一个文件，跟jsp include的作用一样，实现复用的一种方式吧。没有导入/导出，有点不太方便。</p>

<h3>Live Template</h3>

<p>用惯了Eclipse快捷键的人可能会不习惯，sysout、foreach等快捷方式找不到了，main方法也无法自动补全了，其实这个在IntelliJ中有一个异常强大的模块Live Template来实现。</p>

<p>例如，在class中尝试psvm+tab，则会发现main方法产生了；输入iter+tab，则生成了foreach语句。
live template还有一个surround的用法，选中某个变量，键入ctl+alt+j两次，则会出现自动补全的菜单。</p>

<p>此外，还可以自定义Live Template。Code Snippet技术应用也挺普遍的，IntelliJ的Live Template优点是内置了一些智能的变量和函数，可以做到一些语义级别的分析和运用。</p>

<hr />

<h3>几句牢骚</h3>

<p>IDE的圣战从来没有停止过，Eclipse还是IntelliJ好？首先，IntelliJ某些更加体贴的功能，让我感叹一分钱一份货。比如选中括号的后面部分，即使滑动到了下一屏，也会将括号开始的部分浮动显示出来。更重要的，我想引用一下《大教堂与集市》中的比喻，Eclipse好比集市，有开放的环境，本身功能并不求全责备，通过插件来提供相应的功能(最基本的maven、VCS都需要第三方插件提供)。相对的，IntelliJ就像大教堂，内部整合了大多数功能，基本上是一体化的使用设计。</p>

<p>庞大的插件机制和依赖也使得Eclipse出现一些混乱和不稳定。插件依赖/兼容性/稳定性都存在一些问题，而且Eclipse一味向可扩展设计的方式也使得使用起来会更复杂。例如，Eclipse的快捷键设置功能，全部是一字平铺，有一个&#8221;when&#8221;的选项，我理解这是使用快捷键的一个场景。问题是，所有插件都可以向它注册一个场景，当我真要选择when的时候，发现列表有两页之长，我哪知道选哪个？相反，IntelliJ的keymap采用了分类的方式，一级分类就是使用场景，然后再进入相应项设置快捷键，比Eclipse方便的多。再比如，有人说Eclipse慢，其实很可能并不是内核慢，而是一些插件(例如m2e)运行太慢导致的。而IntelliJ基本上都是很迅速的，很少出现失去响应的情况。</p>

<p>这里我想引用一篇文章《有人负责，才有质量：写给在集市中迷失的一代》<a href="http://www.oschina.net/news/32190/a-generation-list-in-the-bazaar">http://www.oschina.net/news/32190/a-generation-list-in-the-bazaar</a>。Eclipse庞大的体系注定了插件管理的松散性，所以使用者就要忍受一些不稳定和不方便的因素。相比IntelliJ，因为是公司开发，大部分插件都在其管理范围之内，所以整体质量更好控制。</p>

<p>说到这里好像就认定IntelliJ好了？其实也未必，因为《大教堂与集市》也提到，开源带来的生产力是教堂式开发远不能比的，所以IntelliJ要收费，而Eclipse可以免费。Eclipse庞大的插件群，功能的全面性，个人觉得也是IntelliJ比不了的。</p>

<p>最后说一句，说到学习成本，其实IntelliJ是要比Eclipse低的，至少省去了很多配置插件、理清依赖、处理问题的功夫，同时设置也比Eclipse要简单不少。没有说越高级的IDE越复杂的说法，只是Eclipse作为最常用的Java IDE，让大家先入为主了罢了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[BlackHole开发日记-使用三种不同IO模型实现一个DNS代理服务器]]></title>
    <link href="http://code4craft.github.com/blog/2013/04/12/blackhole-io-model/"/>
    <updated>2013-04-12T22:16:00+08:00</updated>
    <id>http://code4craft.github.com/blog/2013/04/12/blackhole-io-model</id>
    <content type="html"><![CDATA[<p>BlackHoleJ是一个DNS服务器。他的一个功能是，对于它解析不了的DNS请求，它将请求转发到另外一台DNS服务器，然后再将其响应返回给客户端，起到一个DNS代理的作用。</p>

<p><img src="http://code4craft.github.com/images/posts/forward.png" alt="image" /></p>

<p>这个功能的实现经历了三个版本，也对应了三个经典的IO模型。</p>

<!-- more -->


<h3>BIO模型(Blocking I/O)</h3>

<p>BlackHoleJ代理模式最开始的IO模型，实现很简单，当client请求过来时，新建一个线程处理，然后再线程中调用DatagramChannel发送UDP包，同时阻塞等待，最后接收到结果后返回。</p>

<pre><code>public byte[] forward(byte[] query) throws IOException {
    DatagramChannel dc = null;
    dc = DatagramChannel.open();
    SocketAddress address = new InetSocketAddress(configure.getDnsHost(),
            Configure.DNS_PORT);
    dc.connect(address);
    ByteBuffer bb = ByteBuffer.allocate(512);
    bb.put(query);
    bb.flip();
    dc.send(bb, address);
    bb.clear();
    dc.receive(bb);
    bb.flip();
    byte[] copyOfRange = Arrays.copyOfRange(bb.array(), 0, 512);
    return copyOfRange;
}
</code></pre>

<p>其中dc.receive(bb)一步是阻塞的。因为请求外部DNS服务器往往耗时较长，所以为了达到快速响应，不得不开很多线程进行处理。同时每个线程都需要进行轮询dc.receive(bb)是否可用，会消耗更多CPU资源。</p>

<h3>Select模型(I/O multiplexing)</h3>

<p>BlackHoleJ 1.1开始使用的IO模型。因为DNS使用UDP协议，而UDP其实是无连接的，所以所有请求以及响应复用一个DatagramChannel也毫无问题。同时预先使用DatagramChannel.bind(port)绑定某端口，那么对外部DNS服务器的转发和接收都可以使用这个端口。唯一需要做的就是通过DNS包的特征，来判断到底是哪个客户端的请求！而这个特征也很好选择，DNS包的headerId和question域即可满足需求。</p>

<p>发送方的伪代码大概是这样：</p>

<pre><code>public byte[] forward(byte[] queryBytes) {
    multiUDPReceiver.putForwardAnswer(query, forwardAnswer);
    forward(queryBytes);
    forwardAnswer.getLock.getCondition().await();
    return answer.getAnswer();
}
</code></pre>

<p>接收方是一个独立的线程，代码大概是这样的：</p>

<pre><code>public void receive() {
    ByteBuffer byteBuffer = ByteBuffer.allocate(512);
    while (true) {
        datagramChannel.receive(byteBuffer);
        final byte[] answer = Arrays.copyOfRange(byteBuffer.array(), 0, 512);
        getForwardAnswer(answer).setAnswer(answer);
        getForwardAnswer(answer).getLock.getCondition().notify();
    }
}
</code></pre>

<p>这里forwardAnswer是一个包含了响应结果和一个锁的对象(这里用到了Java的Condition.wait&amp;notify机制，从而使阻塞线程交出控制权，避免更多CPU轮询)。还有一部分是multiUDPReceiver。这里multiUDPReceiver.putForwardAnswer(query, forwardAnswer)实际上是把forwardAnswer注册到一个Map里。</p>

<p>这样做的好处是仅仅在一个线程检查原本的多路I/O是否就绪，也就是I/O multiplexing。这跟Linux下select模型是一样的。</p>

<h3>AIO模型(Asynchronous I/O)</h3>

<p>BlackHoleJ 1.1.3开始，使用了基于回调的AIO模型。这里建立了UDPConnectionResponser对象，里面封装了client的IP和来源端口号。每次收到外部DNS响应时，再根据响应内容找到这个client的IP和来源端口号，重新发送即可。</p>

<p>这实际上就是封装了callback的异步IO。</p>

<p><img src="http://code4craft.github.com/images/posts/aio.png" alt="image" /></p>

<p>发送方的伪代码大概是这样：</p>

<pre><code>public void forward(byte[] queryBytes) {
    multiUDPReceiver.putForwardAnswer(query, forwardAnswer);
    forward(queryBytes);
}
</code></pre>

<p>接收方的代码大概是这样：</p>

<pre><code>public void receive() {
    ByteBuffer byteBuffer = ByteBuffer.allocate(512);
    while (true) {
        datagramChannel.receive(byteBuffer);
        final byte[] answer = Arrays.copyOfRange(byteBuffer.array(), 0, 512);
        getForwardAnswer(answer).getResponser().response(answer);
    }
}
</code></pre>

<p>这里getResponser().response()直接将结果返回给客户端。</p>

<h3>测试：</h3>

<p>使用queryperf进行了测试，使用AIO模型之后，仅仅单线程就达到了40000qps，比1.1.2效率高出了25%，而CPU开销却有了降低。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[2012年总结]]></title>
    <link href="http://code4craft.github.com/blog/2013/02/25/2012-summary/"/>
    <updated>2013-02-25T22:40:00+08:00</updated>
    <id>http://code4craft.github.com/blog/2013/02/25/2012-summary</id>
    <content type="html"><![CDATA[<p>看了别人写的数字总结，自己也来跟风写一个吧。</p>

<p>看了8本书：</p>

<p>《HTTP权威指南》、《UNIX编程艺术》、《程序员的思维修炼》、《软件随想录》、《TCP/IP详解》、《构建高性能Web站点 : 改善性能和扩展规模的具体做法》、《深入Java虚拟机》、《JAVA并发编程实战》</p>

<!-- more -->


<p>学了一门语言：python。</p>

<p>阅读3个项目源码：JDK集合框架和锁框架，qmail源码，JavaMail源码。</p>

<p>Java web后端开发学到技术：</p>

<p>Struts2 iBatis SpringAOP</p>

<p>做了3个得意的业余项目：可配置博客搬家、BlackHole、太极验证码。</p>

<p>写了45篇技术相关的博客，内容的质量比起2011年有了很大提高。</p>

<p>参加了两次20人左右的演讲。</p>

<p>完成了一个大项目：邮件重构，参与者7人。</p>

<p>买了一个域名codecraft.us，做了一个网站。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[BlackHole开发日志--防止DNS污染]]></title>
    <link href="http://code4craft.github.com/blog/2013/02/25/blackhole-anti-dns-poison/"/>
    <updated>2013-02-25T21:13:00+08:00</updated>
    <id>http://code4craft.github.com/blog/2013/02/25/blackhole-anti-dns-poison</id>
    <content type="html"><![CDATA[<h3></h3>

<hr />

<h3>DNS污染原理</h3>

<p>DNS污染是比DNS劫持更加难以防御的一种攻击，受攻击者访问网站时可被导向其他域名，例如某“不存在的网站”被导向了一个“不存在的IP地址”。</p>

<p>DNS污染的原理如下：</p>

<p>DNS查询也是一个经典的请求-回答模式。首先，客户端发起DNS查询，这是一个UDP包。路由器在转发IP包时，对其内容做解析，若发现其是使用53端口的UDP包，并且其内容符合某些特征(普通的DNS请求都是明文)，则从旁路直接返回一个伪造的应答，将其应答指向某个特定IP。因为这个返回速度非常快，所以先于正常请求到达客户端。而客户端收到一个返回包，就认为得到了答案，不再继续接收，而正确的请求结果就被忽略了！</p>

<p><img src="http://code4craft.github.com/images/posts/dns-poison.png" alt="image" /></p>

<!-- more -->


<p>一般防止DNS污染有几种方法：</p>

<ul>
<li><p>修改系统hosts文件</p>

<p>  系统的hosts文件可以配置某个域名对于的IP地址，并且会优先于DNS服务器的响应，所以此方法稳定高效，缺点是host地址需要不断更新。</p></li>
<li><p>改用TCP协议而不是DNS协议发送DNS请求</p>

<p>  DNS也支持TCP协议传输，而TCP协议没有收到污染，所以可以改用TCP作为DNS下层协议。缺点是TCP速度慢，并且DNS服务器支持度有限。代表工具：Tcp-DNS-proxy <a href="https://github.com/henices/Tcp-DNS-proxy">https://github.com/henices/Tcp-DNS-proxy</a></p></li>
<li><p>使用IPv6地址发送DNS请求</p>

<p>  某些站点可以通过IPv6地址访问。代表工具：dnsproxycn <a href="http://code.google.com/p/dnsproxycn/">http://code.google.com/p/dnsproxycn/</a></p></li>
<li><p>根据污染特征过滤伪造DNS答案</p>

<p>  先截获DNS污染结果，然后分析其特征，然后将判断为污染的DNS包过滤掉。代表工具：AntiDnsPollution <a href="http://www.williamlong.info/archives/2184.html">http://www.williamlong.info/archives/2184.html</a></p></li>
</ul>


<h3>BlackHole解决方案</h3>

<p>BlackHole解决方案是第4种：向一个不存在的地址发送DNS查询，正常情况下不会有应答；若触发DNS污染，则只会有伪造包返回。记录这个伪造包的特征(一般是A记录的IP地址)，加入黑名单，下次如果收到这些包，则直接过滤。</p>

<p>其实开发BlackHole时不知道有AntiDnsPollution这款工具，完成了才知道，采取的方法不谋而合。只不过BlackHole做的更复杂一点，增加了一些功能：</p>

<ul>
<li><p>DNS污染黑名单持久化</p>

<p>  所有污染IP都会存入&#8221;安装路径/blacklist&#8221;文件，每次重启可继续读入，也支持手工修改。</p></li>
<li><p>可用IP导出host</p>

<p>  BlackHole会对IP地址做可达性判断(根据ICMP协议请求)，存在DNS污染的域名，若正确DNS地址中，同时有可达IP，则会产生一条&#8221;IP domain&#8221;的DNS记录到&#8221;安装路径/safebox&#8221;文件中，与hosts文件格式一致，可以粘贴进去，从而无须再启动BlackHole。</p></li>
<li><p>支持多DNS服务器请求</p>

<p>  BlackHole可以同时向多个DNS服务器请求，并采用最先返回的正确结果作为答案返回；同时后台会继续接收响应，并根据最终答案中IP地址的可用性进行判断，去掉不可用的IP。你可以将BlackHole的转发DNS配置为：一个ISP提供的服务器，速度较快；另一个权威的DNS服务器，结果较可信。对于大多数请求，ISP提供的DNS可以满足需要，从而降低查找时间。同时如果在公司内网中使用，还可以将公司内部DNS服务器地址配置进去，这样可以保证内部配置的某些DNS的有效性。</p></li>
<li><p>支持缓存</p>

<p>  BlackHole使用ehcache作为缓存，并且可以持久化，下次启动时可直接载入上次缓存结果。</p></li>
<li><p>可配置</p>

<p>  BlackHole还是修改hosts文件的替代方案。通过修改&#8221;config/zones&#8221;可以自定义DNS拦截规则，支持通配符&#8221;*&ldquo;。</p></li>
</ul>


<p>BlackHole的源码地址：<a href="https://github.com/code4craft/blackhole">https://github.com/code4craft/blackhole</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[十年莽撞IT路]]></title>
    <link href="http://code4craft.github.com/blog/2013/02/17/fenng-career/"/>
    <updated>2013-02-17T18:39:00+08:00</updated>
    <id>http://code4craft.github.com/blog/2013/02/17/fenng-career</id>
    <content type="html"><![CDATA[<p>原文地址：<a href="http://dbanotes.net/mylife/ten_years_it_road.html">http://dbanotes.net/mylife/ten_years_it_road.html</a> 作者冯大辉<a href="http://weibo.com/fenng">@fenng</a></p>

<!-- more -->


<p>按：博文视点周筠老师多次叮嘱我写点关于个人成长的心得，颇感压力。回首在 IT 这个行业也差不多有 10 年了，在互联网也有 7 年之久，没做成什么惊人的事业，也没什么赚到大把的钞票，如果冒充什么成功导师大谈人生感悟岂不是会被读者朋友笑掉大牙，当然引来板砖也说不定。如果说有可取的，恐怕也就是自己莽撞的混入这个行业阴差阳错的一路走过来，有些教训或许能供朋友们参考一下。所以，硬着头皮记录一下过去几年的流水账。是为序。</p>

<h3>唯一幸运的事情</h3>

<p>我是东北人，家乡是一个产粮大县，周围上百里都是平原，一望无际的玉米田，春耕秋收的时候倒也非常漂亮。只是小的时候家乡经济并不发达，回想起来仍然没多少好感。从小到大，没吃过什么苦，没享过什么福。小学的时候，有幸遇到了一位很好的启蒙老师，很大程度上启发了我的思维，让我终身受益，现在想起来，真的要感谢他！这么多年回顾一下，我有很多运气不错的时候，比如初中升高中，全校几百个学生，我是唯一一个进入我们这个县级市重点高中的（可想而知教育质量如何之差）。这并不是我如何刻苦或是如何聪慧，只是带着偶然性的幸运而已。至于在 1997 年能被保送上大学，说来也很巧，那一届的同学里其实有一些有力的竞争者，但我是属于平时学习成绩还算相当不错的，而且会考成绩全部优秀，在当时的班主任郑老师的据理力争之下，居然最终保送成功。我曾经一度怀疑是不是自己家里背着我走了后门托了关系，很遗憾，最后证据不足，因为我们家经济上没有实力，也确实没有这方面的关系。只能归结为运气较好，当然还要感谢我的老师对我青眼有加。</p>

<p>我不知道如果我参加了高考人生会怎样？当时的高考是千军万马过独木桥，远非现在这样上大学这么容易。而我尽管其它科目成绩都不错，但是物理成绩是忽高忽低，我之所以走另一条「安全系数」高一点的路，多少也是出于这个担心。没能参加过高考对我来说是一件非常遗憾的事情，以至于好些年经常做梦都是马上要进行高考，或是满头大汗的在答题，然后在惊慌中醒来。扯远了，确定了自己被录取之后，和周围的同学相比，我一下子多出来几个月的空闲时间，非常悠闲的时光，着实看了不少书。我从很小的时候养成了阅读的习惯，只是限于条件，找不到太多的有趣的书可以读，这次算是弥补一下。在我养成的习惯中，可能阅读是唯一的为数不多的好习惯之，这么多年以来，我每年总要给自己留点空闲时间读书，真的是一种乐趣。</p>

<p>很多朋友应该知道我大学的专业学的是生物学，有的时候我说是生物技术。其实到现在我也不知道，我的大学专业到底应该怎么界定，因为我们的班级叫做「生物基地班」，隶属于「国家理科基础科学研究和教学人才培养基地」，听起来挺高端的，当时全国这样的班级并不多，我当初是学校的第一批。在上大学之前并不知道自己对生物兴趣有多大，只是当时只有两个选择，要么选择历史，要么选择生物，考虑到历史属于文科，选择生物似乎更适合一个理科学生，家里对这个也没有多大的意见。当时影响这个选择的是上一年和生物有关的一件轰动世界的大事：克隆羊多莉的诞生。报刊杂志上都是相关的报道。想到自己以后有可能做这方面的事情，还是很令人兴奋的。我甚至还给面试我的老师介绍了一下什么是克隆。当时，我并不知道自己的一些选择会影响我今后人生的道路。</p>

<p>到了大学以后才发现自己并不喜欢这个专业，生物学中有些基础课程是需要死记硬背的，开始的时候还可以硬着头皮将就一下，到后来就随心所欲了，专业课的时候神游天外，到了考试之前才去临阵突击。遗憾的是，这个时候我意识到自己的确并非什么天才，短时间的冲刺根本解决不了什么问题，当然，结果也不是非常的差。但这样下去终归不是问题，大学二年级的时候，干脆选择了逃避，我偷着联系了历史系的基地班，要转系「跳槽」过去，一时间，引起了学院内的轩然大波，因为这是前所未有的事情，再说，所有人都觉得我们这个专业相当的不错，为什么会有人要走？这不是也丢了院系的面子吗？经过系里的老师苦口婆心的劝说，最后我终于洗心革面，答应老师，要好好学习专业课。总算是暂时平静了下来。经过这一次的折腾，自己的「空间」似乎大了一些。去系里的计算机室似乎受到的约束甚至也小了许多。因为属于教育基地班的缘故，当时院系斥巨资兴建了一个计算机室，配备的是 486 的机器，少数是 586 的机器。和我同期前后上大学的朋友应该对当时那种计算机房还有印象，进来都是要穿鞋套的，还要登记什么的，而我们系看管计算机房的老师似乎生怕学生把计算机捣鼓坏了，整天象防贼一样防备着学生多占用上机时间。这个老师肯定也听说过摩尔定律，但他肯定不知道如何提升计算机的利用率，不知道如何发挥计算机的最大价值。在我们毕业的时候，那个机房已几乎无人问津了–因为机器相比之下都太古老了。</p>

<p>{据说，这么多年之后，我已经是学院里的一个异类的传说，很多老师会把我当作一个典型的例子，不好好学习专业也能做点对社会有用的事情，也让师弟们获得了一点空间。}</p>

<p>我在上大学前没怎么接触过计算机，那时候周围的同学中有机会接触计算机的，要么是家庭条件相当好的，要么是游戏迷，幸运的是，当时在电脑上玩游戏的价格对我来说过于昂贵。到了大学有了接触计算机的条件，我当时的兴趣应该也并不是最狂热的，只是好玩而已。因为开的计算机课程都是一些基本的概念，二进制什么的，编程课是 BASIC，老师其实也并没有讲多少内容。上机的时候，我最喜欢做的一件事情大家肯定猜不到：用 Windows 自带的画图程序描摹蔡志忠的漫画。BASIC 我到现在也不喜欢这个语言，总感觉接触不到底层，或许是我的性格使然。当时的计算机图书，谭浩强的大作几乎就是教科书的代名词，遗憾的是，他的C语言的教材，就像是他那本 BASIC 教材用 C 语言翻译了一下。非常偶然的一个机会，在图书馆看到一位师兄拿着一本 Linux 的书，我在图书馆多次看过 Unix 的书籍，苦于周围没有相关的环境没办法深入学习，而 Linux 这个词不知为什么让我印象很深，以至于回到宿舍我还和同学说起来这个事儿。很快，国内陆陆续续有更多关于 Linux 的报道了，而且，可以买到系统介质光盘了，这样就可以偷着在一些机房安装起来，然后偷着练习一会儿。尽管这样，实践的机会仍然比较少，以至于很多 Unix 下的东西，我都是背下来的。现在回想起来，也真够费劲的。我最不能原谅学校的一点就是学生宿舍用电实在是太「节能」了，每天 10 点半熄灯，而且电压还不够，想弄一台电脑放在宿舍内也不可能。至今想起来仍让人愤懑。不知道现在怎么样了，好像还是老样子。有些陈腐的观念，十年前和现在并无不同。</p>

<p>说这些并不是强调我对计算机有多么热爱，对我来说，只是感觉这是一种能够谋生的可能，而且，还比较有趣，对我更有趣的事情无疑更多，但不是所有有趣的事情都能变成一种职业。所以，选择计算机，至少比我所学的生物学要有更大的可行性。现在回想起来，当时所有人都说生物学是21世纪最有前途的学科，我想，那也有可能是 21 世纪后 20 年才会发生的事儿吧，我没有办法等到那一天。</p>

<p>相比在计算机上投入的时间，可能我花在听音乐上的时间更多。可能摇滚算是一个愤青的标配了吧？在大学里我是中国摇滚乐的死忠支持者，四年下来，为了这个爱好倒是耗费了我不少饭钱。但很明显，在中国做音乐很难让你吃饱饭（可能这一两年好了不少)。另外，音乐这东西除了所谓的灵感之外，必须要足够时间的磨练才可以,所谓一万个小时的训练。这么多年回头看看，那帮摇滚人的确不争气，和这个国家一样，挺让人失望的。现在很多音乐人一出场就「同一首歌」的范儿，偶尔看到让人不胜唏嘘 — 这群人不是挺有性格的嘛，怎么都混成了这个样。</p>

<p>互联网对我的影响是逐渐发生的，第一次接触互联网应该是在 1998 年，并没有有其他人说的那种「震撼」的感觉－因为速度实在是太慢了。直到后来自学计算机的过程中通过网络查找各种资料（当然上网费用也逐渐降了下来），才一点点的体会网络的妙处。互联网对我这样的穷孩子来说，是一种信息上的解放。网络让我看到了一个更为辽阔，甚至可以无限延伸的世界，使得我了解到更多荒谬但是又现实的东西，使我认识到更多的可能从而突破自身的障碍，我想我的人生观应该是在这个时候逐渐形成的。如果没有互联网，我会成为一名误人子弟的教师？或是一个不合格的工程师？不管怎样，不会是现在的我。从某种意义上说，我自己，我们这一代人都应该感谢互联网，而能接触到互联网是我们这一代人唯一真正幸运的一件事。</p>

<p>每当被《程序员》杂志约稿的时候，我都要提醒一下自己：我不是程序员。这是真心话，我没有做过一天真正意义上的程序员，尽管我非常想做。我不太喜欢程序员自嘲称自己为「IT民工」或是「码农」什么的，总觉得做这个行业，就要尊重自己的职业才是。我自己并非计算机的科班出身，在大学里也没能积累下足够多的写代码的经验，毕业求职的时候其实是没办法竞争编程开发相关的岗位的，只能走差异化竞争的路线。万幸的是，我认识到IT行业除了开发程其实还有工作岗位可以选择。所以，较早的逆向推演自己能够做哪些事情，并且结合自己的兴趣，在操作系统(Unix)的实践和网络(比如TCP/IP)理论方面下了一番苦功夫，加上一些机缘巧合，最后能够有幸撞入这个行当。在 2000 年左右的时候，专业歧视还是蛮严重的，那个时候几乎绝大多数 IT 公司的校园招聘都会比较严格的限定专业，而像我这样从八竿子打不着的生物学要跨入IT行业的，基本上很少有人理睬。我想我永远都会感谢给我机会的那位面试官。</p>

<p>我只参加了两次招聘宣讲，第一家是华为，但华为根本不看我们这些八竿子打不着的专业来的人。没有应聘上华为，对我来说也是一件幸运的事情，我的个性散漫，在华为这种半军事化管理的公司里面，肯定会闷死。第二次去参加招聘宣讲，完全是抱着领取奖品的心态去的 — 宣讲会上有可能发放 Linux 资料和光盘。很多人递简历给面试官的时候，我发现自己根本没戏，面对科班出身的同学，随便问几个关于数据结构的问题我也不可能比别人回答的好。于是在旁边站着。恰好，有一位女面试官也在旁边站着。</p>

<p>「您也是面试官？请你看看我的简历吧！」 「你为什么不到那边投简历？」 （指排队递交简历的地方） 「那边不可能要我，我不是学这个的。」 「那你为什么还来？」 「我会点别的东西。」 「都有哪些呢？」 「Unix 和 Linux 自己学了很久，TCP/IP 也会点。」 「那你说出10个Unix命令给我」 「这个容易，(说了一堆）」 「再说说 Unix 的各个运行级是怎么回事？」 「（我又说了一堆）」 「嗯，好像真的学过，今天来面试的好像还真没有对这些感兴趣的人…再说说 TCP？」</p>

<p>就这样，这就是我第一份工作的最主要的面试环节(时间久远，细节上可能稍有差异)。我毕业求职没有很高的目标，但对自己有两个基本的要求：到北京工作， 从事 IT 行业。没想到，比我预想的要顺利不少。</p>

<p>招聘我的公司是个中字头的国企，有着较为荣耀的历史，在当时来看，整体上是一家大公司，但具体到每一家分公司，则是不折不扣的小公司，当然也谈不上什么好的公司文化。因为毕业之前没有和公司联系，所以也没有到公司实习，毕业后直接就直接到北京来报到了，行政人员还为是否留下我这个人作了一番斟酌，想来也挺有趣。我们这几个毕业生的岗位是系统工程师，说的直接一些，其实也就是一些 Unix 下的软件的安装和实施，需要到各地出差，对于刚走出校园、毕业前甚至没有出过东三省的我也是一种很好的锻炼，可以了解一下各地的风土人情。这段工作也磨练了我与人打交道的能力，尽管做的还不够好–还不能很好的控制自己的脾气，当然暗地里也吃过不少亏。</p>

<p>没有项目的时候，有大量的时间和几个同事一起学习和实践。就是在这家公司，我在 Unix 操作系统之外，开始选择数据库作为一个学习方向。没有人告诉我应该做 什么样的选择，说老实话，只是看当时招聘数据库管理员的公司给开出的薪水的确都很不错，就误打误撞开始了数年的数据库技术之路。国内当时也出现了一个面向数据库技术的网络论坛，ITPub.net，聚集了一大批数据库技术的爱好者，大家在论坛上分享资料，交流心得，不亦乐乎，结识了不少朋友。如前所述，正是互联网给了我们学习更多知识的可能，否则，只有在具体的应用场景才有可能接触到这些，我也走不到今天。</p>

<p>在这家公司工作了一年多，感觉自己的数据库有了一点基础，能力有了提高，就冒着极大的”风险”跳槽了。说是风险，因为公司的母公司隶属国企，每个毕业生都签署了四年的合同，如果提前离职要对公司进行赔偿，合同上写明总计 2 万五千元，对当时的我来说，这是一笔巨款。当然现在想可能也没什么。对当时的我来说，倍感压力，有点杨白劳遇上黄世仁的感觉。我想现在的毕业生应该很少再面对这种霸王条款了吧。有朋友会说，毕业一年就跳槽，太对不起这家公司了吧？对当时的我，其实也是不得已而为之，加上当时也不是很懂事。</p>

<h3>数据库</h3>

<p>很多朋友知道我，是因为我的曾经在 DBA 这个岗位工作过很久，实际上，我走向 DBA 的岗位径并非一帆风顺的，甚至稍有一些周折。我新入职的这家公司隶属于一家更大的国企期，公司的负责人雄心勃勃，组建了一只不小的数据库技术团队，主攻电信行业商业智能的市场。遗憾的是，最后在商务上并不理想，而项目实施也出现了不小的问题，当然，那是在我离开之后的事情了。在这个团队我只工作了半年左右的时间，并不顺利，也并不开心。问题主要是出在我自己身上，和直接主管的沟通总是有问题，遗憾的是，我当时甚至不认为自己有问题，这是很多初入职场的人的通病，也或许是很多人的通病－总喜欢把责任推在别人身上，而无知的认为自己没有错。我在这家公司的一个收获是看到了余世维的一个讲座视频，应该是给某电信企业做的培训课程吧。虽然现在看，这一套成功学的东西其实没什么可取之处，但当时给我的感觉还是挺震撼的。因为此前，我从来没有考虑过如何修正自身的一些问题。</p>

<p>从这家公司离开后，在接下来的这家公司的经历则颇有戏剧性，我作为数据库管理员入职后没多久正好赶上非典爆发，第一次享受在家里远程办公的乐趣，非典过后上班第一天得知，老板居然把公司卖掉了，就这样，没做什么事情，拿了几个月工资，但是自己好像没赚到什么便宜。考虑到公司并入新的公司后发展方向对自己并不有利，这是一件完全以销售为主导的公司，技术人员只需要做项目实施即可，到处出差，对着说明文档敲命令，时间长了必然会索然无味。所以不得不考虑再次换一份工作。其实这个时候对我自己来说，已经有些苦恼了，我知道频繁的更换工作对一个人的发展的负面影响是很大的，尤其是在刚进入这个行业不到两年的情况下。所幸自己还没有完全进入舒适区，继续的选择不需要太大的挣扎。</p>

<p>我下决心，无论如何下一个工作要更为长久一些。</p>

<p>很快，又找到了一家公司。新的公司规模不大，但总也是一家外企，老板是韩国人，有美国留学的背景。能够进入外企可能是那个时候很多人的一个阶段性目标，怎么说呢，至少我在当时还是很羡慕一些在外企工作的朋友，起码薪水很不错。这个时候，谁有那么长远的眼光不在乎薪水呢？甚至也很少有过来人跟你讲关于发展与职业规划这些事情，基本上是凭着感觉走，只能多观察，多分析别人的经验（网络上有不少过来人会写自己的职业历程，不需要问他们，阅读，分析，进一步对比自己，了解自己，这就足够了）。新公司同事都很有活力，大家关系相处的也都不错。公司制度也比较灵活，我甚至有的时候中午才去公司上班。当然，公司给出了空间，工作起来也都是挺卖力的。</p>

<p>这家公司有两个业务方向，一块是给联通做增值服务，我的工作职责包括在这一部分；另一块是开发手机上的浏览器，这是公司发展的重点，大部分同事也都是做开发的。先在回想起来，这个浏览器太超前了，当时是2004年左右，手机根本没有发展到这个地步，而且，单靠这一款软件，没有上下游的产品支撑，尽管也有想象力，但最后还是没有合适的出路。一年多之后，公司启动了另一个产品，一个在线音乐网站。这个也是老板借鉴韩国的互联网的模式做出的决定。很显然，结局不猜也会知道，这也是个失败的项目。我想起来这个项目甚至有些后悔，错失了一个很好的锻炼机会，如果当初能够多承担一点点责任的话，或许能做得更好也说不定。</p>

<p>我是在加入这家公司之初开始更多的关注起互联网技术，搭建了自己的个人站点，后来尝试写起了 Blog，通过捣鼓（真的是捣鼓）个人站点，一点点的摸索、学习到了更多的东西。我对Web相关的一些技术没有系统的学习过，只是时间长了形成了感觉而已。早期 Blog 技术圈子都是一些很纯粹的技术爱好者，更多的人只是为了分享而写作，为了乐趣而写作。我自己差不多也是这样，学习搭建站点，学习如何写技术文档，首先写的东西要对自己有用，以这个为标准，逐步发现对别人也有点用，进而得到一些正面反馈（也会满足自己的一点的虚荣心），继续总结，继续写，慢慢的，尝试写一些自己不太熟悉的技术领域的分析笔记，记录，总结，时间久了，也就形成一种习惯了。</p>

<p>通过 Blog 这个途径，我慢慢结识了另外的一个技术群体。做我们这个行当的技术人员总会抱怨没什么前途，没什么空间，根据我的观察，社交面太小也是对很多人的一个制约。当你社交面逐渐打开的时候，接触的信息也会越来越多，所谓的机会，其实是相当多的。如果不擅长在那种传统的社交方式，而网络就是技术人员最好的社交媒介。</p>

<h3>杭州阿里五年</h3>

<p>在2004年的时候，一位素未谋面，但是在社区内打过不少交道的朋友邀我加入阿里巴巴，刚刚启动的支付宝数据库没有人维护，服务器压力也不大，哎呀，非常的吸引我。尽管上一年也有类似的机会，但当时感觉一是自己技术未必能撑起来，二是薪酬似乎也很一般，再者阿里巴巴当时的声誉并非很好（竞争对手散布了很多妖魔化阿里巴巴的信息）。经过一年后，我面对这个邀请，忽然觉得可能是个机会，毕竟再一再二不能再三阿，万一错过了呢？去看看也不会损失什么，而且阿里数据库团队已经拥有好几位技术社区上的牛人了，能吸引这么多人才本身也说明公司里肯定有自己的特点。</p>

<p>杭州一行，整个接触下来感觉这帮家伙都太有趣了，坚定了自己「南下」的决心。马云在2004年底发表过一个言论，「2005年将是中国电子商务安全支付年」，让我很受触动，想想做的事情有可能给互联网带来一些改变，那将是让人多么欣喜的事情，一时间我对杭州的这份新工作充满憧憬。另外，当时我对浙江的商业气氛也很感兴趣，「在那边工作几年，学习一下浙江人怎么做生意，然后再回北京」，我用这个说辞说服了女朋友，我现在的太太。</p>

<p>到了杭州之后工作就上了快车道。支付宝当时正面临着一次相当大的业务改造，为了避免对用户的影响，很多操作都要夜里进行，白天还要支持开发团队，前三个月的工作强度之大让我始料未及，又不能临阵逃脱，只能硬扛。到了项目发布之前的时候，连续几个通宵人都熬不住了。正式发布那天遇到了大麻烦悲剧来临，因为之前赶进度而忽视了性能方面的问题，导致发布的时候恰恰性能问题成了拦路虎，整个技术团队都在后面看着你呢，让人一筹莫展，这种情况下要承受的压力可想而知。让我至今感激的是，团队里的其它几位同事在我撑不住的时候顶了上来，直到最后系统上线，大家终于松了一口气。我不知道有多少人在工作中面临过类似的压力，那段时间，每天早晨起床后，我告诉自己，坚持过今天就好了…就这样，一点点的熬了过来。这就像爬山一样，在你非常累的时候，继续向前走几步，再继续走…也就到了山顶。在以后的几年，我甚至遇到更大的压力，但因为有前面的铺垫，心理抗压能力已经加强了许多，反而会逐渐享受这个过程。一旦进入IT这个行业，早晚都会遇到你职业生涯中的种种看似跃不过去的障碍，我所能给出的建议也无非就是「再坚持一下，不知不觉就跨过去了」。</p>

<p>这段时间我甚至强化了另外一个习惯－阅读。每当压力巨大的时候，为了能不失眠，需要切换一下思维，临睡前就强迫自己看一会儿小说，效果还真不错。我不知道自己什么时候练成了快速阅读的能力，应该是长期积累下来的吧，我坚信只要读的足够块，就可以读到更多的东西，有的值得再次阅读的，反复阅读。小的时候接触不到太多的可以看读的东西，所以有些书籍甚至会读个几十遍，最多的一本书恐怕足有上百遍了吧。到了读大学的时候，有了更大的阅读空间，我成了同学中到图书馆借书最勤快的人，以至于到后来图书馆管理员都认识我了，当然还省吃俭用买了更多的书，包括学习一些新技术的时候，我的一个习惯也是会多买几本书对照着看阅读，对我自己来说，收效还是不错的。我比较喜欢有阅读习惯的技术人，当我面试的时候，如果一个技术人很长时间都没有读过一本书，会让我降低对他的评估分数，当然，这是我的个人偏见而已。 应该说，对于我而言说，阅读是一种乐趣，和有些人喜欢电子游戏是一样的。</p>

<p>在杭州的前三年，为工作牺牲了自己的不少量业余时间（正常的工作时间有的时候反而比较空闲，所以，才有可能写一些文章），因为公司随时可能有事情，而且，有事情就不会是小事情，大多数业务都直接涉及到资金数据，稍有不慎，可能就会酿成大祸。我现在非常怀念和同事们通宵发布的那些日子，的确非常辛苦，但其中也有莫大的乐趣。每当即将拂晓的时候，在崔健的音乐声中，看着窗外的渐渐清晰景色，总让人有一种莫名的欣喜，好像我们每个人的前途都光明起来。</p>

<p>公司的业务实在发展的太快，技术要想不拖业务的后腿，也只有跑的更快，这也是那几年我在技术上有点进步的一个主要原因，强迫自己做更多的事情。慢慢的在工作中我意识到，要更好的提供后端的数据支撑能力，不理解整体的技术架构是不行的，这是我开始学习Web架构方面知识的起因。翻看我自己站点早期的关于网站架构的文章，其实没什么技术含量，无非是一些分析各个网站架构的笔记而已。最开始记录的时候我只想写给自己看。有些东西，看过了不一定理解，理解了不一定能写出来；写出来但不一定能说明白，说明白不一定让别人也明白。我只是从初级阶段做起，把看过的东西做个记录，然后通过读者的反馈再做一定的梳理，有机会的话尝试给别人讲讲，把这当成锻炼自己的一个途径。让我始料不及的是，这一类文章受到了很多读者的关注，以至于让我欢欣鼓舞，逐渐写数据库相关的内容越来越少，写Web相关技术的文章越来越多。不知不觉之间完成了又一次技术背景的转换。</p>

<p>在2008年上半年的时候，因为自己自傲莽撞，我再次犯了一次意识上的错误，而被动的导致了一次工作职责上的调整，或者说被放逐了一段时间，临时成了一个「架构师」，也变得相对清闲起来，于是有机会进行一些面向外部的技术交流，塞翁失马，我打开了另一个天地。通过这些交流，也让公司的一些真正的大牛为业界所知，引进了一些技术人才，侧面改善了一下公司在技术社区的形象，这是让我很欣慰的一件事儿。当然，这些招摇过市的事情也会引起一些误解，但是没有人做事情是能面面俱到的，不是吗？</p>

<p>这段时间终于让我的性格稍微成熟了一点。我一直是一个颇有棱角的人，有的时候会碰得一头包。性格中有些我不想改变的地方，我不会尝试改变；有些有必要改变，就必须要稍稍控制一下。很多技术人员在职场不如意，多数都是出于个人性格原因，这是我的个人感想。所以，认清自己的缺点，尝试做一些改变，终归是必要的。并不是磨平所有的棱角，有些棱角适当的藏起来吧。</p>

<p>过了一年多，我又被召回到数据库团队。作为团队的管理者，在新的岗位上有很多东西需要学，也颇有挑战。但我也越来越觉得我所想要做的不是这些事情，再过几年，也无非重复一些以前的事情，将一些业务数据做得更高，支撑能力更强而已。在这个前后，我三十而立了，我结婚了，我的人生观和价值观不可能不发生一些变化。我更想看到自己的一些想法变成现实，我喜欢把通过努力让产品尽快的改进，我热爱互联网而不喜欢金融的刻板，我喜欢社区，喜欢开源文化，喜欢 Twitter… 不过我讨厌终日繁复的会议…朋友们，你们中一定有人听过「家猪」和「野猪」的故事，我发现自己不知什么时候已经成为了一只「野猪」，再也不能变回「家猪」了。</p>

<p>支付宝现在已经发展成为了一家不折不扣的大公司。我觉得我是个幸运的人，亲身经历了一家公司从初创期到发展壮大的过程。作为一个以技术安身立命的人，在这个过程中我观察到、学习到了的东西比什么都重要，我暗自庆幸没有一味低头干活，有的时候也抬头看了一下路，这是我真正收获到的。在阿里巴巴历经五年多，深刻感受到阿里巴巴是一家了不起的企业，有独特的魅力，将来也定然会发展得更好。更令人怀念的是这里有很多优秀的同事，我从他们身上学到很多，和他们一起战斗的日子让我永生难忘。生命中非常重要的五年留在了这里。要感谢的实在太多。</p>

<p>当写这篇文章的时候，我已经在丁香园(DXY.com)工作接近半年了，我很享受创业的状态，而且，这边有更多需要直面的挑战。尽管大学专业学的不够好，但这个背景对我在丁香园的工作还是很好好处的，想来有趣，自己转了一圈居然又回到了这个领域。在做这个决定之前，自己也有过疑惑，我问自己：你有足够的管理经验么？非常遗憾，没有。不过，我还可以继续学习，还可以不断的改进自己，能够帮助团队迅速成长，我的头脑还没有完全僵化，还能够”坚持”下去，而且，和几位合作伙伴之间的协作也很顺畅。所以，我有信心接受一次新的挑战。我的新的决定也得到了妻子的支持与鼓励，我要谢谢她。</p>

<p>如同当初加入支付宝的理想主义一样，我想通过丁香园这个项目，在医疗健康领域，能够给一些人以帮助，让这个环境稍微美好一点。做这些事情，不一定会让自己功成名就，但是有可能让自己心里更加安宁一点，觉得更加踏实一点。不是每个人到这个世界上最后都会变成富有，但如果能够健康生活，能更加快乐一些，这比什么都重要。</p>

<p>或许，几年后我会写一些在丁香园的经历。</p>

<p>我是不是遗漏了什么？回头看这篇文章，我发现仍然少写了很多或许关键的内容，而我似乎潜意识中也将这个过程美化了许多，选择性的遗忘了许多，所以写得轻松了一些。在杭州这几年，因为不适应气候等诸多因素，每年都有几次很严重的关节疼痛发作，痛不欲生，你看，这只是我这十年付出的代价之一，类似的苦楚还有更多。有的时候想，如果我当初不来杭州会怎样？或者说如果我当初不选择IT会怎样？过去这十年中，我做过不少次选择，我根本不知道如果我做另一种选择的话，我现在会是什么样子。大家应该知道电影《黑客帝国》的那一幕，选择红色药丸还是蓝色药丸，所面对的世界将截然不同。</p>

<p>有的时候面对即将毕业的同学对于职业或是人生的困惑，我真的不知道该如何作答，我不知道每一次选择会怎样改变你，每一个个体是不相同的，不可能复制别人的道路，但有一点可以肯定的是，在这个糟糕的时代，我们都将面对更大的压力，会历经更大的痛苦，唯有更加顽强一些，在你快绝望的时候再坚持一下。这是我经历十年莽撞IT路后给自己的一点忠告。</p>

<p>{这是 2011 年写的东西，重新整理，放在这里作为一份记录}</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[blackhole开发日记II]]></title>
    <link href="http://code4craft.github.com/blog/2013/02/17/blackhole-develop-diary2/"/>
    <updated>2013-02-17T15:14:00+08:00</updated>
    <id>http://code4craft.github.com/blog/2013/02/17/blackhole-develop-diary2</id>
    <content type="html"><![CDATA[<h4>2013-2-17</h4>

<p>时隔一个月，再次迎来更新！</p>

<p>过年的时候产生了一个想法，解决DNS劫持和DNS污染的问题：</p>

<p>收集一些可靠DNS服务器列表，使用同一个本地端口向这些服务器挨个发送请求，同时监听这个端口的返回包。拿到第一条返回包作为结果返回，并且继续监听，收集所有返回包，做为判定DNS劫持和DNS拦截的依据，最终将正确结果缓存。实现的方式是NIO，复用DatagramChannel。</p>

<p>经过测试，gfw在实现DNS污染的同时，不会丢弃掉回包，因此此方案是可行的，而且比起UDP转TCP的方式，理论上速度会快很多。</p>

<!-- more -->


<hr />

<h4>2013-2-25</h4>

<p>成功完成了blackhole在windows下的配置。后来网上搜到有人做过差不多的工具AntiDnsPollution<a href="http://xijie.wordpress.com/2011/05/29/%E5%87%A0%E4%B8%AA%E9%98%B2%E6%AD%A2dns%E6%B1%A1%E6%9F%93%E7%9A%84%E6%96%B9%E6%B3%95/">http://xijie.wordpress.com/2011/05/29/%E5%87%A0%E4%B8%AA%E9%98%B2%E6%AD%A2dns%E6%B1%A1%E6%9F%93%E7%9A%84%E6%96%B9%E6%B3%95/</a>，还比这个简单。有点失望，安安心心做好工作才是王道啊。</p>

<p>以后技术方面还是要多积累才行。不过blackhole项目还是有自己的优势的，低延迟、支持缓存、可持久化、可定制都是其优点。好好把任务完成吧。</p>

<hr />

<h4>2013-4-2</h4>

<p>一次故障排查的经过，详解操作系统DNS重试机制</p>

<h3>出现故障</h3>

<p>昨天下午的时候，发现微博有人私信我。原来是一个用户(应该是在oschina上找到了BlackHole)，在公司内网使用了BlackHole作为内部DNS Server(因为BlackHole配置比较简单嘛)，然后发现，在大规模访问量下，会出现浏览器破页的现象，而且越来越严重。</p>

<p>当时第一反应很开心，自己鼓捣的这个东西确实派上了用场。后来跟他沟通，与我偶尔遇到的情况是一样的：浏览网页的时候偶尔会有DNS解析不到，使用nslookup结果无问题，但是在终端下ping显示找不到host。使用sudo killall -HUP mDNSResponder刷新系统缓存后，该请求解析正常。</p>

<h3>初步检查</h3>

<p>怀疑是操作系统缓存了错误的结果。因为BlackHole在实现的时候有一个trick的技巧：在转发请求的时候，因为发送和返回的设计是异步的，所以需要一个key来将请求和响应对应起来。因为使用了Map结构，所以担心条目太多导致内存泄漏，所以直接使用了DNS头的ID作为key，这个ID是一个16位的整数，空间足够小，不用担心泄漏问题。但是特殊情况下，如果ID冲突，甚至可能发生返回错误的响应的问题。</p>

<p>但是后来进行了尝试，对某个请求，返回伪造的相应体，并分别尝试了question区伪造和answer区伪造两个方法。但是发现，操作系统能够检查出错误的响应体并进行过滤，然后再重试！于是，这个假设的故障其实是不存在的。</p>

<h3>按部就班</h3>

<p>感觉“蒙”的方法总是不准确，还是老老实实按部就班的来吧。个人觉得找bug最好的方法就是重现并记录现场，比没头苍蝇乱找，或者一遍一遍看代码靠谱多了。发现chrome自带了一个工具<a href="chrome://net-internals/#dns">net-internel</a>可以查看DNS缓存情况，打开之后发现，确实有部分DNS出现了<em>error: -105 (ERR_NAME_NOT_RESOLVED)</em>的错误。</p>

<p>因为错误情况非常少且跟输入无关，靠debug是行不通了，只能靠log。在程序中将一个DNS query从接收到转发、返回都打印了log。经过测试发现，原来某些请求返回了空结果！</p>

<p>原因是我模仿Servlet的ServletContext机制，使用了一个ThreadLocal来记录一些状态，其中就有一条是：ServerContext.hasRecord()，表示是否已经存在答案体。结果这个设计并未经过仔细推敲，里面存在一个重大问题：ThreadLocal是单个线程对应的上下文，而我在主方法中使用了线程池ThreadPoolExecutor，而实际上线程池的线程是复用的！也就是说我这次请求，会拿到另外请求的上下文，所以有些请求本来没有记录，却当作有记录，结果返回了空响应！而且因为这个ThreadLocal变量没有清理机制，所以后来会有越来越多的空响应。</p>

<p>而DNS server返回空响应(有完整的header和question)也就相当于说，我正常处理了这条请求，但是这个domain是没有数据的，你下次别查了。操作系统的甄别能力是有限的，遇到这个空响应，它就傻傻的相信了，并且缓存了起来。</p>

<p>至此故障成功定位。解决方案：去掉这个半成品的ServerContext，改为参数传递。改天详细研究一下Servlet的上下文保存机制才行。</p>

<h3>总结：操作系统DNS缓存及重试机制</h3>

<p>经过这次排查，也发现了操作系统DNS的一些机制：</p>

<ul>
<li><p>操作系统能够在一定范围内识别不正确的DNS响应(DNS头ID、question区、answer区name错误)，并进行重试。</p></li>
<li><p>操作系统会缓存空记录。</p></li>
<li><p>某些浏览器(例如chrome)会在DNS返回空记录的情况下，让缓存立即过期。这相当于浏览器级别的重试机制。浏览器是能够清除操作系统缓存的。</p></li>
</ul>


<p>经过这番折腾，BlackHole之前一直存在的一个问题也算是解决了。有人把它应用到企业内网，还是挺令人鼓舞的。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mac使用ssh socks代理翻墙]]></title>
    <link href="http://code4craft.github.com/blog/2013/01/05/macshi-yong-ssh-socksdai-li-fan-qiang/"/>
    <updated>2013-01-05T21:46:00+08:00</updated>
    <id>http://code4craft.github.com/blog/2013/01/05/macshi-yong-ssh-socksdai-li-fan-qiang</id>
    <content type="html"><![CDATA[<p>听说最近VPN封的厉害，都改用ssh翻墙了。在Mac下实验了一把，总算鼓捣成功。总结如下：</p>

<!-- more -->


<p>使用ssh作为socks proxy：</p>

<p>有很多软件都可以实现这个功能，例如Secret Socks，SSH Tunnerl Manager。Secret Socks不支持自动连接，同时每次都要输入密码，太麻烦了。SSH Tunnerl Manager也是要输入密码。不过SSH Tunnerl Manager有个好处，更改配置后它会把SSH命令都显示出来。原来Mac下SSH直接就可以支持socks代理，大概这样配置：</p>

<pre><code>ssh -N -p 22 -C -c -D 7070 username@hostname
</code></pre>

<p>但是这样依然无法保存密码，后来搜索到一个软件sshpass，下载编译之，运行却提示找不到/usr/libexec/ssh-askpass，后来从网上找到一个人的解决方案，发现其实这就是提供密码的脚本，于是直接在里面敲上：</p>

<pre><code>#! /bin/sh 

echo yourpassword
</code></pre>

<p>然后使用</p>

<pre><code>sshpass -pyourpassword ssh -N -p 22 -C -c -D 7070 username@hostname
</code></pre>

<p>就可以了！sshpass貌似自带断线重连功能，相当给力！</p>

<p>还有就是autoproxy的配置，在Chrome下使用SwitchySharp，在规则列表里加上<a href="https://autoproxy-gfwlist.googlecode.com/svn/trunk/gfwlist.txt">https://autoproxy-gfwlist.googlecode.com/svn/trunk/gfwlist.txt</a>，选择AutoProxy兼容列表即可，当然对应情景模式别忘了选成你配好的本地代理！</p>

<p>最后将启动脚本ln -s到/Library/StartupItems/，至此大功告成。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[BlackHole开发日记]]></title>
    <link href="http://code4craft.github.com/blog/2012/12/17/blackhole-develop-diary/"/>
    <updated>2012-12-17T23:04:00+08:00</updated>
    <id>http://code4craft.github.com/blog/2012/12/17/blackhole-develop-diary</id>
    <content type="html"><![CDATA[<h3>起因</h3>

<p>最近公司在做一个邮件系统的项目，涉及到测试对外发送的环节。开始构思是这样：建立一个接收服务器，并将所有请求导向该服务器。这里面就涉及到一个DNS拦截的问题。这个问题其实在开发和测试环境中很常见，但是单是绑hosts或者使用传统DNS都不太能满足需要(不支持通配符)。</p>

<p>后来调研DNS工具也烦了，于是想自己写一个，实现简单的功能。找到一个Java的开源项目EagleDNS看了下，把UDP连接模块看了看，发现还是比较简单的。于是就构思着开发一个简单的DNS服务器吧！</p>

<!-- more -->


<hr />

<h3>开发日志</h3>

<hr />

<h4>2012-12-14</h4>

<p>构想功能和研究协议花了一天时间，后来就急不可耐的开始编码了。花了一个上午，做了写死配置，一个拦截所有请求的简单服务器，发现能够work，更加坚定了信心。</p>

<p>下午做了一些开发，参考jetty的思路，实现了handler的结构，基本代码成型。因为不想引入Spring，写了一堆很丑的单例。</p>

<p>晚上回家，引入了Spring，并做了一些bugfix。</p>

<p>引入了一个监控模块wifesays，后来发现Java有个模块JMX就是做这个事的，长见识了！</p>

<hr />

<h4>2012-12-15</h4>

<p>将wifesays独立成一个项目。</p>

<p>兴致勃勃的定义了wifesays的协议，是一个基于TCP的文本协议，端口用的是老婆的生日40310，呵呵。</p>

<p>开发了一个wifesays的客户端，用了apache的commons-cli做参数解析（这是个好东西）。目标是代替telnet，并且可以通过文件输入而不是telnet那样全是纯手工的方式。这样写一个配置文件，就可以达到发送测试邮件的目的。后来Socket超时那部分很难搞定，再说吧。</p>

<p>晚上遇到一个问题：之前的构思是，希望将blackhole作为第一顺序DNS服务器，只负责拦截，然后无法拦截的通过断开连接的方式，使用系统配置的第二顺序DNS服务器解析。</p>

<p>但是后来发现，这样做会非常低效并且不一定可行。</p>

<p>MacOX下，当系统尝试第一顺序DNS失败次数过多后，会放弃尝试，以后的请求都会使用其他DNS服务器。</p>

<p>CentOS下，系统每次请求都会尝试第一顺序DNS服务器，但是超时时间非常长，导致每次解析变得很慢。</p>

<p>后来尝试使用BlackHole做代理，于是有了转发模式。测试之后，发现转发模式能够工作。当时的兴奋很难形容。</p>

<p>目标转移到可用性上来，熬夜做了SHELL脚本，并写了README。事实证明README是很有用的。</p>

<hr />

<h4>2012-12-16</h4>

<p>使用BIND的压力测试工具queryperf做了benchmark。</p>

<p>第一次测试结果不尽人意，拦截模式qps为6000，转发模式只有3000，而BIND有36000。</p>

<p>后来想到会不会是log的原因？因为到了大于10000qps的时候，IO操作耗时就显得很重要了。于是关掉log重试，结果提升明显，拦截模式qps达到16000，转发模式为8000。</p>

<p>BIND是采用C写的，难道Java比C有天生的劣势？忽然想到HotSpot虚拟机都是运行一段时间会变快的，于是尝试多次测试，发现拦截模式qps达到30000。看来Java在工作时间变长之后，性能劣势就并非那么明显了。</p>

<p>benchmark的优秀滋生了将BlackHole做成一个通用DNS服务器的野心。这是一个很宏伟的目标，涉及到DNS协议完全分析、缓存机制、UDP协议分析等。是个很有前途的目标，come on!</p>

<hr />

<h4>2012-12-17</h4>

<p>今天在公司公开了这个项目，得到大家的肯定，坚定了把这个项目做下去的决心。</p>

<p>下午公司项目codereview，被指出很多问题。虽然自己在代码可扩展性上做了不少努力，但是大家都反应可读性不那么好。决定以后改进风格。这个决定也体现在BlackHole上，因为一开始就想用英文写代码注释，所以多看看JavaDoc也是很有必要的！锻炼下英文吧，感觉这也是开源的必经之路。</p>

<p>晚上回家时间不多，尝试着将ehcache引入，结果效果让人大跌眼镜，qps直接降到3000。不知道ehcache做了什么事，感觉额外的东西太多。</p>

<p>但是缓存依然是需要的，得日后调研了，或许自己写一个。</p>

<p>开始构思的是缓存外部DNS的UDP包内容，后来发现Message.getHeader()存在一个ID，如果该ID不符，则可能导致不正确的结果。queryperf中出现了很多这样的错误：</p>

<pre><code>Warning: Received a response with an unexpected (maybe timed out) id: 3
</code></pre>

<p>看来详细研究一下DNS是非常有必要的。</p>

<p>晚上开始记录开发日志。这才是货真价实的“每天进步一点！”。</p>

<hr />

<h4>2012-12-18</h4>

<p>早上做了ehcache的benchmark，set和get一个40k的字符串(压缩到19k)到ehcache。10000次set和get操作，每次都使用不同的key。</p>

<p>ehcache的测试结果(10000次)：</p>

<table>
    <tr>
        <td width="100">Operation</td>
        <td width="100">Total(ms)</td>
        <td>Average(ms)</td>
    </tr>
    <tr>
        <td>set</td>
        <td>34</td>
        <td>0.0034</td>
    </tr>
    <tr>
        <td>get</td>
        <td>6</td>
        <td>0.00065</td>
    </tr>
</table>


<p>结果相当令人满意。</p>

<p>作为对比，使用JDK提供的简单Cache类型WeakHashMap做一个测试(10000次)：</p>

<table>
    <tr>
        <td width="100">Operation</td>
        <td width="100">Total(ms)</td>
        <td>Average(ms)</td>
    </tr>
    <tr>
        <td>set</td>
        <td>4</td>
        <td>4e-4</td>
    </tr>
    <tr>
        <td>get</td>
        <td>5</td>
        <td>5e-4</td>
    </tr>
</table>


<p>可以看到，ehcache的set比WeakHashMap耗时提升了10倍，但是get的效率是差不多的。因为cache的使用场景，一般都是读远大于写，所以ehcache是一个胜任的进程内缓存。回去需要调研一下昨天失败的原因。</p>

<p>出于兴趣，又使用memcached做了一个测试。spy的客户端似乎会将字符串做压缩，最后将40k的字符串压缩到19k，但是结果仍然是毫秒级的，这也证明了涉及到网络IO的操作，会慢上几个数量级。</p>

<p>memcached的测试结果(1000次)：</p>

<table>
    <tr>
        <td width="100">Operation</td>
        <td width="100">Total(ms)</td>
        <td>Average(ms)</td>
    </tr>
    <tr>
        <td>set</td>
        <td>4796</td>
        <td>4.796</td>
    </tr>
    <tr>
        <td>get</td>
        <td>1726</td>
        <td>1.726</td>
    </tr>
</table>


<p>开始调研DNS协议。尝试输出DNS query的UDP包内容，发现不是文本编码，看来得研究一下了。</p>

<p>晚上到家，尝试将MX记录也伪造一下，方法是在domain前面加上mail.，并且把mail.domain记录到A记录列表，下次将这个mail.domain直接指向配置好的IP地址。queryperf真是个好东西。</p>

<p>晚上使用dig查看了一些知名网站的TTL，发现都在几百到几千之间。后来尝试了一下MacOX下对TTL的应对策略，发现不会又TTL设置那么长，但是绝对跟TTL有关系。</p>

<p>突然有个想法，可以将BlackHole内置缓存，并且可以主动刷新！这样子就很方便了，因为如果是本地搭建BlackHole，实际上本地做缓存和使用BlackHole缓存效率是相近的。</p>

<p>明天的任务是解析代理的响应，并做缓存。</p>

<hr />

<h4>2012-12-19</h4>

<p>今天上班的路上在对比Tomcat和Jety的连接模型。Tomcat使用多线程处理请求，一个请求一个线程；Jetty则采用NIO。有文章说，对于逻辑复杂、处理时间较长的连接，Tomcat有优势，但是对于处理时间较短的连接，Jetty有优势。毫无疑问DNS的处理时间是很短的，看来把之前connector部分多线程的处理模型换成NIO的，应该性能会有一些提升。以后几天一个大的开发计划，就是调研Java NIO和Jetty里的实现，写一个完整的NIO connector。Cache机制开发延后。</p>

<p>上午在网上找了点资料，磕磕碰碰把基于NIO的服务器模型搭出来了。因为DNS使用的是UDP协议，同时数据量很小，所以NIO并没有起到理想中的效果。</p>

<p>主要服务器部分代码在<a href="https://github.com/flashsword20/blackhole/blob/nio/src/main/java/us/codecraft/blackhole/connector/UDPSocketMonitor.java">https://github.com/flashsword20/blackhole/blob/nio/src/main/java/us/codecraft/blackhole/connector/UDPSocketMonitor.java</a>。测试之后的效果让人失望，NIO效率比多线程+BIO更低，只有24000~28000qps，并且因为代码不熟悉，系统变得很不稳定。所以暂时保留到NIO分支了。</p>

<p>结论是这样的：</p>

<p>一个DNS query的包大小大约是几十byte，而response也不超过100 byte，如此小的传输量，同时因为UDP包也不涉及连接建立等东西，发送速度也很快，所以NIO发挥不了效率。</p>

<p>晚上回家，尝试加入cache机制：</p>

<p>使用了ehcache，但是死活都存在UDP包乱序的问题。后来发现</p>

<p><strong>对于同一key，ehcache获取出来的对象总是同一个，这也是进程内缓存的特殊之处，不妨将其想想为一个Map</strong></p>

<p>而我会尝试改变其ID值，结果就导致看起来ID变了，其实也改变了其他线程获取到的值，出现了不确定性！</p>

<p>后来改进了数据结构，尝试写一个CopyOnWriteMessage(Message是dnsjava中DNS报文的抽象类)，结果因为其很多方法都是私有，所以失败了。后来构建了一个UDPPackage的对象，尝试将byte[]用CopyOnWrite和访问锁的方式解决并发问题，测试结果CopyOnWrite性能会好一点，这也是因为数据量较小，而且Arrays.copyOf是本地方法吧。</p>

<p>后来测试，值得高兴的是有缓存的情况下，转发模式效率达到了40000qps，可喜可贺！</p>

<p>家里连公司，benchmark里，UDP丢包率大概在10%，效果相当差，所以说网络才是第一要素！</p>

<hr />

<h4>2012-12-20</h4>

<p>今天尝试使用Selector改造转发逻辑，结果可耻的失败了！因为<strong>Selector不是线程安全的</strong>，试图多个线程进行register会导致严重的问题，这也是为什么基于事件的IO模型都不怎么支持多线程的原因，太难做了。</p>

<p>后来尝试用Java的Future机制来实现超时。我们知道，Java的FutureTask需要一个执行的线程池。如果每次都new出来当然没有问题，但是经测试性能开销较大(qps被降到了4000)。后来尝试复用同一大线程池，发现请求多了之后，总是会超时！</p>

<p>然后jstack查看发现，很多线程仍然卡在了Callable.call方法，就是我们常说的异常把线程抛死了！原来TimeoutException也会导致线程抛出异常但是无法回收。解决方法：使用future.cancel()。</p>

<p>这也说明，线程抛出异常死掉时，jstack查看到的是它抛出异常时的执行栈。</p>

<p>加入超时机制后，程序算是稳定了，这周末弄个1.0版吧，以后开始写ReleaseNotes。</p>

<p>今天看了一下代码，作为一个开源项目，似乎风格不是那么优雅，很多长函数，注释也不完整。顶多对架构和原理感兴趣，代码的OOP神马的，这方面完全提不起兴趣来啊。</p>

<p>在自己的MBP上也编译了一个queryperf，测试性能达到55000qps，而拦截模式只有35000，果然还是缓存byte[]更给力啊。因此把拦截模式也加入了cache。</p>

<p>晚上实验BlackHole作为DNS实装了！DNS设置为127.0.0.1，转发DNS设为之前的DNS。目前表现稳定。</p>

<hr />

<h4>2012-12-21</h4>

<p>今天，太阳照常升起，逃过了一劫，那就开始新的生命吧。</p>

<p>为了把BlackHole推广出去，想要做一个MacOS下的包。调研了mac开机启动的东西，将启动程序写成了一个plist文件，放在/Library/LaunchDaemons下面，脚本是这样的：</p>

<pre><code>&lt;?xml version="1.0" encoding="utf-8"?&gt;
&lt;!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd"&gt;
&lt;plist version="1.0"&gt;
  &lt;dict&gt;
      &lt;key&gt;Label&lt;/key&gt;
      &lt;string&gt;blackhole.init&lt;/string&gt;
      &lt;key&gt;ProgramArguments&lt;/key&gt;
      &lt;array&gt;
        &lt;string&gt;/usr/local/blackhole/blackhole-start.sh&lt;/string&gt;
      &lt;/array&gt;
        &lt;key&gt;KeepAlive&lt;/key&gt;
        &lt;false/&gt;
      &lt;key&gt;RunAtLoad&lt;/key&gt;
        &lt;true/&gt;
        &lt;key&gt;StandardErrorPath&lt;/key&gt;
        &lt;string&gt;/tmp/blackhole.err&lt;/string&gt;
        &lt;key&gt;StandardOutPath&lt;/key&gt;
        &lt;string&gt;/tmp/blackhole.out&lt;/string&gt;
        &lt;key&gt;UserName&lt;/key&gt;
        &lt;string&gt;root&lt;/string&gt;
  &lt;/dict&gt;
&lt;/plist&gt;
</code></pre>

<p>事实证明UserName=root那段必须加上，要不然不会用root权限启动，然后就会失败！</p>

<p>重启后一切正常，开始使用BlackHole作为DNS服务器，考察稳定性。</p>

<p>早上来到公司就出事了，主进程启动了，wifesays没启动，导致telnet无响应。悲剧！</p>

<p>需要加入多个DNS服务器的配置。还有就是启动的时候，为什么wifesays老是启动不起来？</p>

<p>后来知道，因为<strong>Spring初始化时是阻塞进行的，包括afterPropertySet方法，如果里面调用了耗时较长的程序，则会阻塞直到服务结束才会继续初始化，而这个过程的顺序是未知的</strong>。ehcache初始化的时间挺久(好多秒，不知道为什么)，然后就导致wifesays的进程一直等不到Spring初始化结束，这时调用之，就抛出了空指针异常！后来将ehcache做了异步加载，并进行了错误判断，问题解决！</p>

<p>下午使用qmail进行拦截测试的时候出了点问题，后来debug发现qmail竟然打了一条ANY类型的请求！原来还有这种类型，长见识了。</p>

<p>找到几个公用的DNS服务器，以后可以增加多服务器切换的功能。</p>

<p>多服务器的可用性是个问题，刚好最近也在研究服务器的可用性维护。最终实现如下：使用failedTimes和wakeup机制。failedTimes由外部调用指定，当次数过多时，标志为不可用；内部线程定期循环，尝试检查不可用的服务器，一旦可用，则重新标志为可用。</p>

<hr />

<h4>2012-12-22</h4>

<p>今天做了一天家务，周末比上班还忙啊。晚上9点半开始写了点代码，想把外部DNS选择机制改为：最短请求时间。</p>

<p>晚上从9点开始写代码，终于到12点写完了。开始尝试用TreeMap做一个按照响应时间自排序的数据结构，类似redis的SortedSet。后来发现：如果尝试改变TreeMap的key值，TreeMap并不会重新排序，到后面就会出现无法预知的结果了！还有一个办法就是写一个incrScore的函数，并且限定score为double类型，但是这时score就没法用自定义类型了，也不是很好用。</p>

<p>所以最后干脆在弹出时做了一个筛选，每次选出所有元素，并做一个筛选。后来这个算法就能正常运行了。</p>

<hr />

<h4>2012-12-23</h4>

<p>今天下午写了一点代码，尝试把超时跟平均响应时间分开了，分为正常响应时间、次数和超时次数。如何才能做到多个服务器可用性的判断？这个问题有很多种方案，希望找到一个可行有效的方案。</p>

<p>这又涉及到业务的东西了。上班写一堆业务，下班不想写业务啊写业务。想做一个使用Selector，多个DNS服务器同时forward，使用最快的结果作为响应。但是后来测试，用途不大。</p>

<p>自己在这里研究这些意义不大，这应该是一个很common的问题。晚上研究一下load balance和failover的东西。</p>

<p>感觉这几天写的load balance的东西都太不成熟，需要等研究一下相关机制之后，再来继续深入。</p>

<p>今天也没有想到其他的优化点了。开发先到这里吧，以后做一点推广方面的工作吧，写一个站点，然后把东西挂上去。老薛主机的下载速度太慢了，看来要找个地方把文件放进去。</p>

<hr />

<h4>2012-12-24</h4>

<p>今天写的不能叫开发日记了，主要是学习，主要想要学习一些load balance的东西。</p>

<p>看了LVS社区的一些东西，系统涉及的目标：透明性、可伸缩性、高可用性和易管理性，跟我想的还不太一样。</p>

<p>VIP技术：无缝的单点切换。</p>

<p>这里看了一篇章文嵩博士自己发表的<a href="http://zh.linuxvirtualserver.org/node/98">LVS集群系统网络核心原理分析</a>，其中转发的原理有三种模式，可以理解为都是修改IP包内容，然后使用户与Real Server实现通信，看上去就像直接和Load Balancer通信一样。</p>

<p>调度算法才是我想研究的重点，LVS实现了八中负载均衡调度算法，总结下来分为三种策略：
轮循调度(Round-Robin)，最小连接数和哈希。</p>

<hr />

<h4>2012-12-25</h4>

<p>今天想了想，如果把BlackHole作为一个拦截工具的话，一直开启DNS服务器也不合适，所以就想写一个模块，在启动时修改OS的DNS配置，关闭时再把DNS配置切换回去，看看是不是可行？这个工具我另建了一个项目叫<a href="https://github.com/flashsword20/dnstools">dnstools</a>。实现了一些windows下的DNS切换，用的是Java外部命令调用。</p>

<p>研究了下Mac下DNS的修改方法。Mac下虽然有/etc/resovled.conf文件，但是这个文件是自动生成的，好像也不起什么作用。后来搜到一个帖子<a href="http://hints.macworld.com/article.php?story=20050621051643993">Configure DNS lookups from the terminal </a>讲的比较详细。我大概整理下帖子的内容：</p>

<p>首先，Mac下网络配置存在/Library/Preferences/SystemConfiguration/preferences.plist这个文件中，每次系统重启会读取这个文件。但是尝试直接修改文件后，发现配置不能立刻生效，搜索之后，也没有找到好的办法使它立刻生效。</p>

<p>还有一个办法是使用scutil。scutil修改的结果在下次重启后会失效，希望持久化修改，请修改/Library/Preferences/SystemConfiguration/preferences.plist文件。网上还搜到一篇关于使用scutil修改DNS的病毒，很好玩。这里贴上帖子里的一段修改DNS的代码，感谢原作者<a href="http://hints.macworld.com/users.php?mode=profile&amp;uid=1035540">emzy</a>(10.7下测试通过)：</p>

<pre><code>#!/bin/bash

# Script is used to set the Nameserver Lookup under Max OS X 10.4 with the Console
# Script by Stephan Oeste &lt;stephan@oeste.de&gt;

if [ $# -lt 2 ] ; then
echo "Use: $0 &lt;domain&gt; &lt;1.Nameserver&gt; [2.Nameserver]"
echo "Example Use: $0 example.tld 1.2.3.4 1.2.3.5"
exit 1
fi

PSID=$( (scutil | grep PrimaryService | sed -e 's/.*PrimaryService : //')&lt;&lt; EOF
open
get State:/Network/Global/IPv4
d.show
quit
EOF
)

scutil &lt;&lt; EOF
open
d.init
d.add ServerAddresses * $2 $3 
d.add DomainName $1
set State:/Network/Service/$PSID/DNS
quit
EOF
</code></pre>

<hr />

<h4>2012-12-26</h4>

<p>今天完成了Windows下DNS的读取和写入，用的是ipconfig /all和netsh，在Java中调用脚本大概是这样子的：</p>

<pre><code>Process exec = Runtime.getRuntime().exec(
            commandString));
BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(exec.getInputStream()));
</code></pre>

<p>bufferedReader读出来就是程序的输出。只要Java程序使用管理员权限启动，那么调用的脚本也会获得对应权限，在MacOS和Win7下测试通过。</p>

<p>今天仔细研究了下scutil，发现大概是个读写plist的工具。Mac下的配置好像都是基于XML的。</p>

<pre><code>例如：

open
d.init
d.add ServerAddresses * $2 $3 
d.add DomainName $1
set State:/Network/Service/$PSID/DNS
</code></pre>

<p>这一段中，d其实是个XML的值，scutil里称之为&#8221;dict&#8221;。如果要获取一个dict的值，就是这么写：</p>

<pre><code>open
get State:/Network/Service/$PSID/DNS
d.show
</code></pre>

<p>发现另一款类似的软件<a href="http://thekelleys.org.uk/dnsmasq/doc.html">dnsmasq</a>，也是用于本地的轻量级DNS服务器，似乎是从hosts文件读取配置，并加以拦截，用于解决某些情况下修改hosts不生效的问题。看来已经有人研究过这个了！不过无论如何，做个东西总是好的。正好学到一些思想什么的。</p>

<hr />

<h4>2012-12-27</h4>

<p>今天将Mac下DNS设置的模块完成了，于是着手做一个单机服务器吧！将项目拆开成了两个目录，server和localserver。</p>

<p>另外解决了一个很初级的Spring配置的问题，如果要引入jar包中的配置，需要在classpath后面加上&#8217;<em>&lsquo;，例如：
classpath</em>:/spring/applicationContext*.xml</p>

<p>后来又遇到一个问题：想要把shell脚本打入jar包，但是发现即使获取到了jar包中shell脚本的路径(xxx.jar!/xx/ss这样的路径)，也无法使用外部程序调用这个文件！</p>

<hr />

<h4>2012-12-28</h4>

<p>怎么执行jar包内的shell脚本？这是个有趣的话题。后来尝试使用getResourceAsStream读取出文件，然后再写到临时文件夹，然后访问…好吧，问题解决。</p>

<p>后来完善了localserver的设计。考虑到项目已经有4个模块了，就引入了maven聚合来完成编译,将xml中module的路径配置一下就可以了。 顺便提一下，maven-jar-plugin也挺好用的，可以将META-INF写入jar包。例如，下面设置依赖路径和执行的Main类:</p>

<pre><code>&lt;plugin&gt;
    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
    &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;
    &lt;configuration&gt;
        &lt;archive&gt;
            &lt;manifest&gt;
                &lt;addClasspath&gt;true&lt;/addClasspath&gt;
                &lt;classpathPrefix&gt;lib&lt;/classpathPrefix&gt;
                &lt;mainClass&gt;us.codecraft.blackhole.selfhost.MacMain&lt;/mainClass&gt;
            &lt;/manifest&gt;
        &lt;/archive&gt;
    &lt;/configuration&gt;
&lt;/plugin&gt;
</code></pre>

<hr />

<h4>2012-12-29</h4>

<p>今天完成了Mac下的单机服务器版本，可以在程序启动的时候设置DNS服务器、清空DNS缓存，结果遇到了一个比较麻烦的问题：即使系统缓存清空了，浏览器仍然会有缓存。而且有个有趣的事情：<strong>浏览器DNS缓存的时间跟ttl值无关</strong>，因为浏览器不知道A记录的TTL值，所以一旦访问成功，都会尝试用一个固定过期时间来缓存内容。因为这个问题还挺费解的，也给之前的开发和测试带来不少困扰，所以就做了一个详细的研究，写了一篇博文：<a href="http://my.oschina.net/flashsword/blog/99068">为什么修改hosts不立即生效？&mdash;浏览器DNS缓存机制分析</a>。</p>

<p>剩下的就是一些操作的包装了，好好考虑和测试一下。</p>

<h4>2012-12-30</h4>

<p>今天继续写单机版BlackHole。碰到一个问题，想要将程序作为系统进程，后台运行，并且在shell关闭时不退出，有两种办法：一种是使用nohup，一种是使用Deamon程序的开发方式来写Java，并引入很多框架，例如Apache Commons Daemon。后者觉得太重了，但是前者无法在控制台输出一些错误信息，也不够友好。最后用了一个很粗暴的方法：将错误输出重定向到一个文件，shell脚本退出的时候打印出来！赢了！</p>

<p>后来使用package maker做了一个安装包，比想象中好用，支持shell脚本什么的。</p>

<p>遇到一个问题，在mac下设置两个DNS，BlackHole为主DNS，结果仍然无法保证每次都使用BlackHole进行解析。后来索性改成只有一个DNS，127.0.0.1，倒是正常工作了。</p>

<p>这个单机版本我取名叫hostd，大概是hosts取代者的意思吧。因为要保证程序即使被kill之后也能做出一些释放资源的操作(在hostd里，需要把修改过的DNS改回来)，所以给wifesays增加了一个响应，用了Java里一个响应信号量的api。</p>

<pre><code>import sun.misc.Signal;  
import sun.misc.SignalHandler; 
Signal.handle(new Signal("TERM"), new SignalHandler() {

        @Override
        public void handle(Signal arg0) {
            shutDown();
        }

});
</code></pre>

<h4>2012-12-31</h4>

<p>新年的最后一天，大家都无心上班，那么我就在上班时间鼓捣项目了！将hostd完善了一下，加入了实时响应配置更改的机制。</p>

<p>开始的想法是通过md5来判断文件内容是否被更改，但是这样每隔一个周期就必须完全load一次文件，不划算；后来想到，为什么不直接用文件的最后修改时间呢？大多数情况下，只要是人工修改的配置文件，多个配置文件的修改时间是不可能相同的，在Java里直接可以用file.lastModified()来查看，多方便！</p>

<p>晚上写了一篇广告帖，效果不好，大家都去过节去了嘛。</p>

<p>后来有个以前搞手机的同事回复我，手机上目前没有方便的hosts修改工具。android下改hostd是需要重启的，相当麻烦。于是感觉找到一个很大的应用场景了！</p>

<h4>2013-1-1</h4>

<p>新的一年，新的日期格式，稍微有点不习惯。今天抽空看了一点Android开发的东西，鼓捣了一下adt。网易有个公开课讲android的，个人认为讲的不错，正好学点英语，地址<a href="http://v.163.com/special/opencourse/developingandroidapplications.html">密西西比河谷州立大学：Android应用程序开发</a>。
<a href="http://masl.cis.gvsu.edu/2010/10/12/android-dev-tutorial-now-available-on-the-gvsu-itunes-u-portal/">原文</a>，<a href="http://masl.cis.gvsu.edu/wp-content/uploads/2010/10/AndroidTutorialHandout.pdf">讲义</a>，<a href="http://masl.cis.gvsu.edu/wp-content/uploads/2010/10/AndroidTutorialSourceCode.zip">源码</a></p>

<h4>2013-1-2</h4>

<p>今天白天出门了，晚上把<a href="http://v.163.com/special/opencourse/developingandroidapplications.html">Android公开课</a>看了两集。虽然这个课程是比较浅，不过详略还算得当，总得来说还算是不错的，而且只有3个小时，对这种速成式的比较感兴趣。后来浏览了几个国内的视频，大多数都是面向零基础的，而且面面俱到，讲的比较慢。因为自己也没打算把这个当正业，加上也有些Java和Swing的基础，了解下大概就可以先试试开发了吧。</p>

<h4>2013-1-3</h4>

<p>hostd for Android项目正式启动！目标是在Android下动态修改域名绑定，不需要修改hosts，不需要修改DNS服务器，也不需要重新切换APN，以用于开发环境和线上环境的快速切换。</p>

<p>因为博主是个猴急的开发者，秉承快速原型的的原则，今天开始了一些开发的尝试。</p>

<p>因为DNS服务器BlackHole要使用系统端口53，所以首先要确认其在Android上是否能够运行。部署服务艰难重重，记录如下：</p>

<p>第一次：</p>

<p>新建一个helloworld Android项目，在buildPath里加入依赖jar包，然后在MainActivity.onCreate()直接启动DNS服务器。尝试第一次，不成功：</p>

<p>解决：使用adb logcat查看，发现产生了NoClassDefinedError，检查APK包，发现依赖jar没有打进去。再次检查buildPath，在&#8221;Order and Export&#8221;选项里把这些jar包都勾上，然后jar包被打到APK里。</p>

<p>第二次：</p>

<p>因为在BlackHole中使用了Spring，所以出现了问题：ClassPathXmlApplicationContext解析不到xml文件路径：提示找不到对应bean。</p>

<p>解决：改用FileSystemXmlApplicationContext解析，并将xml存入临时目录/sdcard/spring.xml。结果大跌眼镜，抛出异常：</p>

<pre><code>Unable to validate using XSD: Your JAXP provider [org.apache.harmony.xml.parsers.DocumentBuilderFactoryImpl@461a0cd0] does not support XML Schema. Are you running on Java 1.4 with Apache Crimson? Upgrade to Apache Xerces (or Java 1.5) for full XSD support.
</code></pre>

<p>难道Android的xmlparser不支持xsd?感觉不太可能啊，猜测应该是Spinrg底层使用XML parser和Android不兼容(Android包里似乎使用了SAXParser)。于是放弃Spring，看看以后是不是用其他IoC框架了，比如Spring for Android?</p>

<p>第三次：</p>

<p>直接启动SocketServer，绑定53端口，进行尝试。</p>

<p>问题如期而至，显示Unable to bind port，Perimission denied</p>

<p>解决：根据网络上的解决方案，在程序中插入这一段，理论上可以在运行到这里时，提示需要root权限：</p>

<pre><code>    try {
        Runtime.getRuntime().exec("su");
    } catch (IOException e) {
        e.printStackTrace();
    } 
</code></pre>

<p>结果：
没有出现提示，依然没有权限！</p>

<p>第四次：</p>

<p>继续搜索资料，发现模拟器没有root权限！作为一个玩安卓不刷机的人，还真不知道该怎么root。后来下载了一个<a href="http://ishare.iask.sina.com.cn/f/20228587.html">Root.apk</a>，尝试之，终于弹出Superuser窗口(经典的骷髅头s)。</p>

<p>后来分析，Android root的原理，就是将su替换成另外一个文件，并且使用Superuser来管理这些权限。真是曲折！</p>

<h4>2013-1-4</h4>

<p>后来发现SocketException是没有网络权限，BindException才是没有系统权限！</p>

<p>尝试root，发现hosts什么的还是不能改，明天再搞吧！</p>

<h4>2013-1-5</h4>

<p>今天咨询了一个做Android的朋友，原来su只能赋予被外部执行的程序的权限，而app本身并不会因此获得权限。所以app中想要使用1024以下端口会失败。朋友他们执行需要root权限的操作，都是使用su加上外部命令来做。没有想象中的那么简单，而且Android环境不熟，开发起来各种调不通，很恼人，看来需要很多时间投入，最近也挺忙的，有大项目要上线了，因此hostd for Android暂时搁置，等有空了再搞吧。</p>

<p>今天买了一个翻墙工具，了解到了gfw通过DNS污染来拦截的事情。可以使用BlackHole来做一个防止DNS污染的小工具？</p>

<p>最近要做的东西好多啊，暂时就这样吧。BlackHole写了有三周了，业务时间每天没有间断过，过程中也有不少提升。找机会再更新吧。</p>

<hr />

<h4>2013-1-16</h4>

<p>gfw的实现机制，包括两项重要技术：DNS劫持及DNS污染。</p>

<p>DNS劫持的原理跟BlackHole一模一样，只不过它是配置到ISP给的DNS服务器那里的。或许gfw掌握了一个比BlackHole更全面的DNS拦截服务器版本，呵呵。这个可以通过自行配置DNS服务器达到避免DNS劫持的效果，比如：配置成google dns: 8.8.8.8。</p>

<p>DNS污染的原理是：不对DNS请求做拦截，而是在利用DNS解析的漏洞进行伪造。因为DNS请求是一个UDP包，假设用户向8.8.8.8发出DNS请求，实际上就是发送了一个UDP包，8.8.8.8收到请求后，返回一个答案UDP包，用户拿到第一个收到的UDP包，当成正确答案。而DNS污染则是监控到一个DNS的UDP包后，从旁路路由返回一个伪造的UDP包。因为这个伪造的包一般比较快，所以会被用户的系统当成正确答案，从而达到伪造的目的。</p>

<p>因为BlackHole是自己实现的转发和判断机制，那么完全可以防范DNS劫持和DNS污染。实现之后，或许会有更广泛的应用场景。这个列入开发计划了！</p>

<h4>持续更新</h4>

<hr />

<h3>心得</h3>

<p>这个项目选择相当有挑战，基本上就是自己开发一个服务器。很重要的一个心得：从零开始开发，开始尝试将需要的组件都自己简单实现，后面再引入框架级的东西，可以更好的理解J2EE世界的工具。</p>

<p>比如开始准备实现一个简单的GlobalFactory，做Spring做的事，只是省去繁琐的xml配置（而且印象中Spring启动实在是太慢了）。后来发现，依赖管理是相当复杂的一块，特别是初始化的时候的顺序。后来只好引入了Spring，发现Spring本身启动并不慢，而且使用annotation代替xml之后，也相当容易配置，重构起来也很方便，一改我在公司项目开发中，Spring又慢又笨重的印象。</p>

<p>再比如项目需要做到一个可外部管理应用的东西，当时还得意洋洋的搞了个项目叫wifesays，用的是TCP文本协议。后来才发现，Java有个模块JMX，专门就是用来干这事的。不过之前很难理解JMX，现在发现容易理解很多。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[程序员每天每周每月每年应该做的事情]]></title>
    <link href="http://code4craft.github.com/blog/2012/12/12/coder-routine/"/>
    <updated>2012-12-12T20:11:00+08:00</updated>
    <id>http://code4craft.github.com/blog/2012/12/12/coder-routine</id>
    <content type="html"><![CDATA[<p><em>【引自等你 我的朋友的博客】作为一个合格程序员每天该做的事</em></p>

<!-- more -->


<h4>程序员每天该做的事</h4>

<h5>1、总结自己一天任务的完成情况</h5>

<p>最好的方式是写工作日志，把自己今天完成了什么事情，遇见了什么问题都记录下来，日后翻看好处多多；</p>

<h5>2、考虑自己明天应该做的主要工作</h5>

<p>把明天要做的事情列出来，并按照优先级排列，第二天应该把自己效率最高的时间分配给最重要的工作；</p>

<h5>3、考虑自己一天工作中失误的地方，并想出避免下一次再犯的方法</h5>

<p>出错不要紧，最重要的是不要重复犯相同的错误，那是愚蠢；</p>

<h5>4、考虑自己一天工作完成的质量和效率能否还能提高</h5>

<p>一天只提高1%，365天你的效率就能提高多少倍你知道吗？(1+0.01)*365 = 37 倍；</p>

<h5>5、看一个有用的新闻网站或读一张有用的报纸，了解业界动态</h5>

<p>闭门造车是不行的，了解一下别人都在做什么，对自己能带来很多启示；</p>

<h5>6、记住一位同事的名字及其特点</h5>

<p>你认识公司的所有同事吗？你了解他们吗？</p>

<h5>7、清理自己的代码</h5>

<p>今天完成的代码，把中间的调试信息，测试代码清理掉，按照编码风格整理好，注释都写好了吗？</p>

<h5>8、清理自己的桌面</h5>

<p>当日事当日毕，保持清洁干净的桌面才能让你工作时不分心，程序员特别要把电脑的桌面清理干净；</p>

<h4>程序员每周该做的事</h4>

<h5>1、向你的老板汇报一次工作</h5>

<p>让你的老板知道你在做什么，这很重要。可以口头、书面、邮件，看你老板的工作方式而定；</p>

<h5>2、进行一次自我总结（非正式）</h5>

<p>这周之内自己表现得怎么样？该加分还是扣分？</p>

<h5>3、制定下周计划</h5>

<p>把下周要做的事情列出来，一样要分清楚优先级；</p>

<h5>4、整理自己的文件夹、书柜和电脑文件</h5>

<p>把桌面以外的地方也要清理干净，电脑的文件夹，收到的邮件，把过时的垃圾全部清理掉；</p>

<h5>5、与一个非公司的朋友沟通</h5>

<p>它山之石，可以攻玉；</p>

<h5>6、看一本杂志</h5>

<p>找一本适合自己的专业杂志；</p>

<h5>7、纠正自己或同事一个细节上的不正确做法</h5>

<p>《细节决定成败》看过了吗？没看过强烈建议先看看；</p>

<h4>程序员每月该做的事</h4>

<h5>1、至少和一个同事一起吃饭或喝茶</h5>

<p>不光了解自己工作伙伴的工作，还要了解他们的生活；</p>

<h5>2、自我考核一次</h5>

<p>相对正式地考核自己一下，你对得起这个月的工资吗？</p>

<h5>3、对你的同事考核一次</h5>

<p>你的同事表现怎么样？哪些人值得学习，哪些人需要帮助？</p>

<h5>3、制定下月的计划</h5>

<p>确定下月的工作重点；</p>

<h5>4、总结自己工作质量改进状况</h5>

<p>自己的质量提高了多少？</p>

<h5>5、有针对性地对一项工作指标做深入地分析并得出改进的方案</h5>

<p>可以是对自己的，也可以是对公司的，一定要深入地分析后拿出自己的观点来。要想在老板面前说得上话，做的成事，工作上功夫要做足；</p>

<h5>6、与老板沟通一次</h5>

<p>最好是面对面地沟通，好好表现一下自己，虚心听取老板的意见，更重要的是要了解老板当前关心的重点。</p>

<h4>程序员每年该做的事</h4>

<h5>1、年终总结</h5>

<p>每个公司都会做的事情，但你真正认真地总结过自己吗？</p>

<h5>2、兑现给自己、给家人的承诺</h5>

<p>给老婆、儿子的新年礼物买了没有？给自己的呢？</p>

<h5>3、下年度工作规划</h5>

<p>好好想想自己明年的发展目标，争取升职/加薪、跳槽还是自己出来干？</p>

<h5>4、掌握一项新技术</h5>

<p>至少是一项，作为程序员一年要是一项新技术都学不到手，那就一定会被淘汰。掌握可不是看本书就行的，要真正懂得应用，最好你能够写一篇教程发表到你的blog；</p>

<h5>5、推出一种新产品</h5>

<p>可以是一个真正的产品，也可以只是一个类库，只要是你创造的东西就行，让别人使用它，也为世界作点贡献。当然如果真的很有价值，收点注册费也是应该的。</p>
]]></content>
  </entry>
  
</feed>
